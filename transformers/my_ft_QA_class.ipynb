{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e67094a5-9255-46e1-bb87-d4afbd87face",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers 微调语言模型-问答任务\n",
    "\n",
    "**注意：微调后的模型仍然是通过提取上下文的子串来回答问题的，而不是生成新的文本。**\n",
    "\n",
    "### 模型执行问答效果示例\n",
    "\n",
    "![Widget inference representing the QA task](docs/images/question_answering.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f78885a-8e62-4756-a3d3-3ffffcfcb242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据你使用的模型和GPU资源情况，调整以下关键参数\n",
    "squad_v2 = False\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad0bf71-8c5c-47b1-b6d2-0099e1b256c6",
   "metadata": {},
   "source": [
    "## 下载数据集\n",
    "\n",
    "在本教程中，我们将使用[斯坦福问答数据集(SQuAD）](https://rajpurkar.github.io/SQuAD-explorer/)。\n",
    "\n",
    "### SQuAD 数据集\n",
    "\n",
    "**斯坦福问答数据集(SQuAD)** 是一个阅读理解数据集，由众包工作者在一系列维基百科文章上提出问题组成。每个问题的答案都是相应阅读段落中的文本片段或范围，或者该问题可能无法回答。\n",
    "\n",
    "SQuAD2.0将SQuAD1.1中的10万个问题与由众包工作者对抗性地撰写的5万多个无法回答的问题相结合，使其看起来与可回答的问题类似。要在SQuAD2.0上表现良好，系统不仅必须在可能时回答问题，还必须确定段落中没有支持任何答案，并放弃回答。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e3dbc63-1b8d-440d-96a4-877806292fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")\n",
    "# The datasets object itself is DatasetDict, which contains one key for the training, validation and test set.\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2562c75-da8b-476e-8dce-3acad6328103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9052a27f-85e9-447b-ba1e-14b4acf3d7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f2999ec-a898-4c0c-8f4c-8eb861e1ee4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56d1204617492d1400aab9fa</td>\n",
       "      <td>New_York_City</td>\n",
       "      <td>Uniquely among major American cities, New York is divided between, and is host to the main branches of, two different US district courts: the District Court for the Southern District of New York, whose main courthouse is on Foley Square near City Hall in Manhattan and whose jurisdiction includes Manhattan and the Bronx, and the District Court for the Eastern District of New York, whose main courthouse is in Brooklyn and whose jurisdiction includes Brooklyn, Queens, and Staten Island. The US Court of Appeals for the Second Circuit and US Court of International Trade are also based in New York, also on Foley Square in Manhattan.</td>\n",
       "      <td>What federal district court has its main courthouse in Brooklyn?</td>\n",
       "      <td>{'text': ['the Eastern District of New York'], 'answer_start': [349]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>572698f9dd62a815002e8aa2</td>\n",
       "      <td>Somerset</td>\n",
       "      <td>The main coastal towns are, from the west to the north-east, Minehead, Watchet, Burnham-on-Sea, Weston-super-Mare, Clevedon and Portishead. The coastal area between Minehead and the eastern extreme of the administrative county's coastline at Brean Down is known as Bridgwater Bay, and is a National Nature Reserve. North of that, the coast forms Weston Bay and Sand Bay whose northern tip, Sand Point, marks the lower limit of the Severn Estuary. In the mid and north of the county the coastline is low as the level wetlands of the levels meet the sea. In the west, the coastline is high and dramatic where the plateau of Exmoor meets the sea, with high cliffs and waterfalls.</td>\n",
       "      <td>What are the main coastal towns</td>\n",
       "      <td>{'text': ['Minehead, Watchet, Burnham-on-Sea, Weston-super-Mare, Clevedon and Portishead'], 'answer_start': [61]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5731d25ae17f3d140042244c</td>\n",
       "      <td>Pacific_War</td>\n",
       "      <td>In the early hours of 7 December (Hawaiian time), Japan launched a major surprise carrier-based air strike on Pearl Harbor without explicit warning, which crippled the U.S. Pacific Fleet, leaving eight American battleships out of action, 188 American aircraft destroyed, and 2,403 American citizens dead. At the time of the attack, the U.S. was not officially at war anywhere in the world, which means that the people killed or property destroyed at Pearl Harbor by the Japanese attack had a non-combatant status.[nb 11] The Japanese had gambled that the United States, when faced with such a sudden and massive blow, would agree to a negotiated settlement and allow Japan free rein in Asia. This gamble did not pay off. American losses were less serious than initially thought: The American aircraft carriers, which would prove to be more important than battleships, were at sea, and vital naval infrastructure (fuel oil tanks, shipyard facilities, and a power station), submarine base, and signals intelligence units were unscathed. Japan's fallback strategy, relying on a war of attrition to make the U.S. come to terms, was beyond the IJN's capabilities.</td>\n",
       "      <td>How many American battleships were put out of action in the attack on Pearl Harbor by Japan?</td>\n",
       "      <td>{'text': ['eight'], 'answer_start': [196]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57283b36ff5b5019007d9f9e</td>\n",
       "      <td>Dissolution_of_the_Soviet_Union</td>\n",
       "      <td>On October 25–28, 1990, Rukh held its second congress and declared that its principal goal was the \"renewal of independent statehood for Ukraine\". On October 28 UAOC faithful, supported by Ukrainian Catholics, demonstrated near St. Sophia’s Cathedral as newly elected Russian Orthodox Church Patriarch Aleksei and Metropolitan Filaret celebrated liturgy at the shrine. On November 1, the leaders of the Ukrainian Greek Catholic Church and of the Ukrainian Autocephalous Orthodox Church, respectively, Metropolitan Volodymyr Sterniuk and Patriarch Mstyslav, met in Lviv during anniversary commemorations of the 1918 proclamation of the Western Ukrainian National Republic.</td>\n",
       "      <td>Where was the UAOC protest held?</td>\n",
       "      <td>{'text': ['near St. Sophia’s Cathedral'], 'answer_start': [223]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570baa0f6b8089140040f9ce</td>\n",
       "      <td>Infrared</td>\n",
       "      <td>IR data transmission is also employed in short-range communication among computer peripherals and personal digital assistants. These devices usually conform to standards published by IrDA, the Infrared Data Association. Remote controls and IrDA devices use infrared light-emitting diodes (LEDs) to emit infrared radiation that is focused by a plastic lens into a narrow beam. The beam is modulated, i.e. switched on and off, to encode the data. The receiver uses a silicon photodiode to convert the infrared radiation to an electric current. It responds only to the rapidly pulsing signal created by the transmitter, and filters out slowly changing infrared radiation from ambient light. Infrared communications are useful for indoor use in areas of high population density. IR does not penetrate walls and so does not interfere with other devices in adjoining rooms. Infrared is the most common way for remote controls to command appliances. Infrared remote control protocols like RC-5, SIRC, are used to communicate with infrared.</td>\n",
       "      <td>What is the IrDA?</td>\n",
       "      <td>{'text': ['the Infrared Data Association'], 'answer_start': [189]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>570bfabb6b8089140040fb00</td>\n",
       "      <td>ASCII</td>\n",
       "      <td>Probably the most influential single device on the interpretation of these characters was the Teletype Model 33 ASR, which was a printing terminal with an available paper tape reader/punch option. Paper tape was a very popular medium for long-term program storage until the 1980s, less costly and in some ways less fragile than magnetic tape. In particular, the Teletype Model 33 machine assignments for codes 17 (Control-Q, DC1, also known as XON), 19 (Control-S, DC3, also known as XOFF), and 127 (Delete) became de facto standards. The Model 33 was also notable for taking the description of Control-G (BEL, meaning audibly alert the operator) literally as the unit contained an actual bell which it rang when it received a BEL character. Because the keytop for the O key also showed a left-arrow symbol (from ASCII-1963, which had this character instead of underscore), a noncompliant use of code 15 (Control-O, Shift In) interpreted as \"delete previous character\" was also adopted by many early timesharing systems but eventually became neglected.</td>\n",
       "      <td>When was paper tape popular?</td>\n",
       "      <td>{'text': ['until the 1980s'], 'answer_start': [264]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>57278fba5951b619008f8d90</td>\n",
       "      <td>Hindu_philosophy</td>\n",
       "      <td>The Yoga school builds on the Samkhya school theory that jñāna (knowledge) is a sufficient means to moksha. It suggests that systematic techniques/practice (personal experimentation) combined with Samkhya's approach to knowledge is the path to moksha. Yoga shares several central ideas with Advaita Vedanta, with the difference that Yoga is a form of experimental mysticism while Advaita Vedanta is a form of monistic personalism. Like Advaita Vedanta, the Yoga school of Hindu philosophy states that liberation/freedom in this life is achievable, and this occurs when an individual fully understands and realizes the equivalence of Atman (soul, self) and Brahman.</td>\n",
       "      <td>To what school does yoga share central ideas?</td>\n",
       "      <td>{'text': ['Advaita Vedanta'], 'answer_start': [291]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>56e3c21939bdeb14003478ec</td>\n",
       "      <td>Estonian_language</td>\n",
       "      <td>The birth of native Estonian literature was in 1810 to 1820 when the patriotic and philosophical poems by Kristjan Jaak Peterson were published. Peterson, who was the first student at the then German-language University of Dorpat to acknowledge his Estonian origin, is commonly regarded as a herald of Estonian national literature and considered the founder of modern Estonian poetry. His birthday on March 14 is celebrated in Estonia as the Mother Tongue Day. A fragment from Peterson's poem \"Kuu\" expresses the claim reestablishing the birthright of the Estonian language:</td>\n",
       "      <td>When was Peterson born?</td>\n",
       "      <td>{'text': ['March 14'], 'answer_start': [401]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>570db44716d0071400510d0c</td>\n",
       "      <td>Adolescence</td>\n",
       "      <td>There are certain characteristics of adolescent development that are more rooted in culture than in human biology or cognitive structures. Culture has been defined as the \"symbolic and behavioral inheritance received from the past that provides a community framework for what is valued\". Culture is learned and socially shared, and it affects all aspects of an individual's life. Social responsibilities, sexual expression, and belief system development, for instance, are all things that are likely to vary by culture. Furthermore, distinguishing characteristics of youth, including dress, music and other uses of media, employment, art, food and beverage choices, recreation, and language, all constitute a youth culture. For these reasons, culture is a prevalent and powerful presence in the lives of adolescents, and therefore we cannot fully understand today's adolescents without studying and understanding their culture. However, \"culture\" should not be seen as synonymous with nation or ethnicity. Many cultures are present within any given country and racial or socioeconomic group. Furthermore, to avoid ethnocentrism, researchers must be careful not to define the culture's role in adolescence in terms of their own cultural beliefs.</td>\n",
       "      <td>Should culture be directly connected to a nation or ethnicity?</td>\n",
       "      <td>{'text': ['not'], 'answer_start': [954]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>56d45abf2ccc5a1400d830e9</td>\n",
       "      <td>Beyoncé</td>\n",
       "      <td>At age eight, Beyoncé and childhood friend Kelly Rowland met LaTavia Roberson while in an audition for an all-girl entertainment group. They were placed into a group with three other girls as Girl's Tyme, and rapped and danced on the talent show circuit in Houston. After seeing the group, R&amp;B producer Arne Frager brought them to his Northern California studio and placed them in Star Search, the largest talent show on national TV at the time. Girl's Tyme failed to win, and Beyoncé later said the song they performed was not good. In 1995 Beyoncé's father resigned from his job to manage the group. The move reduced Beyoncé's family's income by half, and her parents were forced to move into separated apartments. Mathew cut the original line-up to four and the group continued performing as an opening act for other established R&amp;B girl groups. The girls auditioned before record labels and were finally signed to Elektra Records, moving to Atlanta Records briefly to work on their first recording, only to be cut by the company. This put further strain on the family, and Beyoncé's parents separated. On October 5, 1995, Dwayne Wiggins's Grass Roots Entertainment signed the group. In 1996, the girls began recording their debut album under an agreement with Sony Music, the Knowles family reunited, and shortly after, the group got a contract with Columbia Records.</td>\n",
       "      <td>Who signed the girl group on October 5, 1995?</td>\n",
       "      <td>{'text': ['Dwayne Wiggins's Grass Roots Entertainment'], 'answer_start': [1126]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60b6ee7-f2fc-4922-a9b6-7d6272d64cdd",
   "metadata": {},
   "source": [
    "## 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61c5a9cd-f6cf-4275-a686-ae6fa10a5969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b6eec7-7182-4d14-a11d-9d6a29fab3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "# 以下断言确保我们的 Tokenizers 使用的是 FastTokenizer（Rust 实现，速度和功能性上有一定优势）。\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3c327b4-b349-4f6b-8721-556574325ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2054, 2003, 2115, 2171, 1029, 102, 2026, 2171, 2003, 25353, 22144, 2378, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 您可以直接在两个句子上调用此标记器（一个用于答案，一个用于上下文）\n",
    "tokenizer(\"What is your name?\", \"My name is Sylvain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a7757f-38a1-4c6c-aa67-57d10edc02d4",
   "metadata": {},
   "source": [
    "### Tokenizer 进阶操作\n",
    "\n",
    "在问答预处理中的一个特定问题是如何处理非常长的文档。\n",
    "\n",
    "在其他任务中，当文档的长度超过模型最大句子长度时，我们通常会截断它们，但在这里，删除上下文的一部分可能会导致我们丢失正在寻找的答案。\n",
    "\n",
    "为了解决这个问题，我们允许数据集中的一个（长）示例生成多个输入特征，每个特征的长度都小于模型的最大长度（或我们设置的超参数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfe37479-b2af-4921-ae17-a9e47e60650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length of a feature (question and context)\n",
    "max_length = 384 \n",
    "# The authorized overlap between two part of the context when splitting it is needed.\n",
    "doc_stride = 128 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1533a1a-773c-4c5b-a4bf-77522d48e239",
   "metadata": {},
   "source": [
    "#### 超出最大长度的文本数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d38d6e8-dc54-40c3-a4ef-4660ebe0708b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们从训练集中找出一个超过最大长度（384）的文本\n",
    "for i, example in enumerate(datasets[\"train\"]):\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > max_length:\n",
    "        break\n",
    "# 挑选出来超过384（最大长度）的数据样例\n",
    "example = datasets[\"train\"][i]\n",
    "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f5913b-d8f3-4e84-a7f3-3a7a6b59542c",
   "metadata": {},
   "source": [
    "#### 截断上下文不保留超出部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cedcbac1-5dfa-4baa-a1d3-2c6502977692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"],\n",
    "              example[\"context\"],\n",
    "              max_length=max_length,\n",
    "              truncation=\"only_second\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb9233-2e35-4a93-bd30-f7525ee79681",
   "metadata": {},
   "source": [
    "#### 关于截断的策略\n",
    "\n",
    "- 直接截断超出部分: truncation=`only_second`\n",
    "- 仅截断上下文（context），保留问题（question）：`return_overflowing_tokens=True` & 设置`stride`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ac399e5-2e2f-4b91-ba12-7c3747d65b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f0d25dc-1a00-4ae5-89dc-542bc5f51bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[384, 157]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用此策略截断后，Tokenizer 将返回多个 input_ids 列表。\n",
    "[len(x) for x in tokenized_example[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ea20ca3-a093-447e-9617-3c5be9963399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] how many wins does the notre dame men's basketball team have? [SEP] the men's basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla's record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla's 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 – 2010 season. the team is coached by mike brey, who, as of the 2014 – 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were [SEP]\n",
      "[CLS] how many wins does the notre dame men's basketball team have? [SEP] championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were the most by the fighting irish team since 1908 - 09. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 解码两个输入特征，可以看到重叠的部分\n",
    "for x in tokenized_example[\"input_ids\"][:2]:\n",
    "    print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d8d197-a500-408e-a73c-de131c31b438",
   "metadata": {},
   "source": [
    "#### 使用 offsets_mapping 获取原始的 input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3a263d1-63f4-4f37-a37f-e7ae31829de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0]\n",
      "[(0, 0), (0, 3), (4, 8), (9, 13), (14, 18), (19, 22), (23, 28), (29, 33), (34, 37), (37, 38), (38, 39), (40, 50), (51, 55), (56, 60), (60, 61), (0, 0), (0, 3), (4, 7), (7, 8), (8, 9), (10, 20), (21, 25), (26, 29), (30, 34), (35, 36), (36, 37), (37, 40), (41, 45), (45, 46), (47, 50), (51, 53), (54, 58), (59, 61), (62, 69), (70, 73), (74, 78), (79, 86), (87, 91), (92, 96), (96, 97), (98, 101), (102, 106), (107, 115), (116, 118), (119, 121), (122, 126), (127, 138), (138, 139), (140, 146), (147, 153), (154, 160), (161, 165), (166, 171), (172, 175), (176, 182), (183, 186), (187, 191), (192, 198), (199, 205), (206, 208), (209, 210), (211, 217), (218, 222), (223, 225), (226, 229), (230, 240), (241, 245), (246, 248), (248, 249), (250, 258), (259, 262), (263, 267), (268, 271), (272, 277), (278, 281), (282, 285), (286, 290), (291, 301), (301, 302), (303, 307), (308, 312), (313, 318), (319, 321), (322, 325), (326, 330), (330, 331), (332, 340), (341, 351), (352, 354), (355, 363), (364, 373), (374, 379), (379, 380), (381, 384), (385, 389), (390, 393), (394, 406), (407, 408), (409, 415), (416, 418)]\n",
      "[(0, 0), (0, 3), (4, 8), (9, 13), (14, 18), (19, 22), (23, 28), (29, 33), (34, 37), (37, 38), (38, 39), (40, 50), (51, 55), (56, 60), (60, 61), (0, 0), (1093, 1105), (1105, 1106), (1107, 1110), (1111, 1115), (1115, 1116), (1116, 1118), (1119, 1123), (1124, 1133), (1134, 1137), (1138, 1145), (1146, 1152), (1153, 1159), (1160, 1166), (1167, 1172), (1173, 1175), (1176, 1179), (1180, 1187), (1187, 1188), (1189, 1193), (1194, 1195), (1196, 1202), (1203, 1205), (1206, 1208), (1208, 1209), (1209, 1210), (1210, 1211), (1212, 1214), (1214, 1216), (1216, 1217), (1217, 1218), (1219, 1224), (1225, 1233), (1234, 1236), (1236, 1237), (1237, 1240), (1241, 1247), (1247, 1248), (1249, 1252), (1253, 1254), (1255, 1261), (1261, 1262), (1262, 1267), (1268, 1274), (1275, 1277), (1278, 1281), (1282, 1285), (1286, 1290), (1290, 1291), (1292, 1298), (1299, 1302), (1303, 1307), (1307, 1308), (1308, 1310), (1311, 1317), (1317, 1318), (1319, 1322), (1323, 1327), (1328, 1332), (1333, 1335), (1335, 1336), (1336, 1337), (1338, 1341), (1342, 1345), (1346, 1349), (1350, 1353), (1354, 1364), (1365, 1375), (1375, 1376), (1377, 1382), (1383, 1392), (1393, 1395), (1396, 1399), (1400, 1405), (1406, 1407), (1407, 1408), (1409, 1414), (1415, 1418), (1419, 1427), (1428, 1433), (1434, 1438), (1439, 1441), (1442, 1443), (1444, 1450), (1451, 1455), (1455, 1457), (1457, 1458), (1458, 1462), (1462, 1464), (1465, 1472), (1473, 1477), (1478, 1488), (1489, 1497), (1497, 1498), (1499, 1502), (1503, 1505), (1506, 1509), (1510, 1515), (1516, 1521), (1522, 1524), (1524, 1528), (1529, 1534), (1535, 1538), (1539, 1542), (1543, 1546), (1546, 1548), (1548, 1552), (1552, 1554), (1554, 1555), (1556, 1559), (1560, 1568), (1569, 1574), (1575, 1579), (1580, 1583), (1584, 1592), (1593, 1601), (1602, 1610), (1611, 1615), (1616, 1620), (1621, 1627), (1628, 1633), (1634, 1640), (1641, 1644), (1645, 1651), (1651, 1652), (1653, 1656), (1657, 1659), (1660, 1664), (1665, 1669), (1670, 1673), (1674, 1678), (1679, 1681), (1682, 1685), (1686, 1694), (1695, 1700), (1701, 1705), (1706, 1711), (1712, 1716), (1716, 1717), (1717, 1719), (1719, 1720), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "# 设置 `return_offsets_mapping=True`，将使得截断分割生成的多个 input_ids 列表中的 token，通过映射保留原始文本的 input_ids。\n",
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=doc_stride\n",
    ")\n",
    "print(tokenized_example[\"overflow_to_sample_mapping\"])\n",
    "print(tokenized_example[\"offset_mapping\"][0][:100])\n",
    "print(tokenized_example[\"offset_mapping\"][1][:200])\n",
    "# 输出结果：第一个标记（[CLS]）的起始和结束字符都是（0, 0），因为它不对应问题/答案的任何部分，然后第二个标记与问题(question)的字符0到3相同."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6fc9b55-56db-4408-b441-11c46c849305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_token:\n",
      "how How\n",
      "\n",
      "second_token:\n",
      "many many\n"
     ]
    }
   ],
   "source": [
    "# 因此，我们可以使用这个映射来找到答案在给定特征中的起始和结束标记的位置。\n",
    "print(\"first_token:\")\n",
    "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])\n",
    "\n",
    "print(\"\\nsecond_token:\")\n",
    "second_token_id = tokenized_example[\"input_ids\"][0][2]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][2]\n",
    "print(tokenizer.convert_ids_to_tokens([second_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "341d49b5-6103-46ef-944b-15d93986f3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How many wins does the Notre Dame men's basketball team have?\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"question\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2882d0a-8fc8-49e7-b091-b247e6fe6b7d",
   "metadata": {},
   "source": [
    "我们只需区分偏移的哪些部分对应于问题，哪些部分对应于上下文。\n",
    "\n",
    "借助`tokenized_example`的`sequence_ids`方法，我们可以方便的区分token的来源编号：\n",
    "\n",
    "- 对于特殊标记：返回None，\n",
    "- 对于正文Token：返回句子编号（从0开始编号）。\n",
    "\n",
    "综上，现在我们可以很方便的在一个输入特征中找到答案的起始和结束 Token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2faf1b00-3c6d-45b4-b76f-1a3f2b1d1dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 2129, 2116, 5222, 2515, 1996, 10289, 8214, 2273, 1005, 1055, 3455, 2136, 2031, 1029, 102, 1996, 2273, 1005, 1055, 3455, 2136, 2038, 2058, 1015, 1010, 5174, 5222, 1010, 2028, 1997, 2069, 2260, 2816, 2040, 2031, 2584, 2008, 2928, 1010, 1998, 2031, 2596, 1999, 2654, 5803, 8504, 1012, 2280, 2447, 5899, 12385, 4324, 1996, 2501, 2005, 2087, 2685, 3195, 1999, 1037, 2309, 2208, 1997, 1996, 2977, 2007, 6079, 1012, 2348, 1996, 2136, 2038, 2196, 2180, 1996, 5803, 2977, 1010, 2027, 2020, 2315, 2011, 1996, 16254, 2015, 5188, 3192, 2004, 2120, 3966, 3807, 1012, 1996, 2136, 2038, 23339, 1037, 2193, 1997, 6314, 2015, 1997, 2193, 2028, 4396, 2780, 1010, 1996, 2087, 3862, 1997, 2029, 2001, 4566, 12389, 1005, 1055, 2501, 6070, 1011, 2208, 3045, 9039, 1999, 3326, 1012, 1996, 2136, 2038, 7854, 2019, 3176, 2809, 2193, 1011, 2028, 2780, 1010, 1998, 2216, 3157, 5222, 4635, 2117, 1010, 2000, 12389, 1005, 1055, 2184, 1010, 2035, 1011, 2051, 1999, 5222, 2114, 1996, 2327, 2136, 1012, 1996, 2136, 3248, 1999, 4397, 10601, 26429, 10531, 1006, 2306, 1996, 9493, 1052, 1012, 11830, 2415, 1007, 1010, 2029, 11882, 2005, 1996, 2927, 1997, 1996, 2268, 1516, 2230, 2161, 1012, 1996, 2136, 2003, 8868, 2011, 3505, 7987, 3240, 1010, 2040, 1010, 2004, 1997, 1996, 2297, 1516, 2321, 2161, 1010, 2010, 16249, 2012, 10289, 8214, 1010, 2038, 4719, 1037, 29327, 1011, 13913, 2501, 1012, 1999, 2268, 2027, 2020, 4778, 2000, 1996, 9152, 2102, 1010, 2073, 2027, 3935, 2000, 1996, 8565, 2021, 2020, 7854, 2011, 9502, 2110, 2040, 2253, 2006, 1998, 3786, 23950, 1999, 1996, 2528, 1012, 1996, 2230, 1516, 2340, 2136, 5531, 2049, 3180, 2161, 4396, 2193, 2698, 1999, 1996, 2406, 1010, 2007, 1037, 2501, 1997, 2423, 1516, 1019, 1010, 7987, 3240, 1005, 1055, 3587, 3442, 2322, 1011, 2663, 2161, 1010, 1998, 1037, 2117, 1011, 2173, 3926, 1999, 1996, 2502, 2264, 1012, 2076, 1996, 2297, 1011, 2321, 2161, 1010, 1996, 2136, 2253, 3590, 1011, 1020, 1998, 2180, 1996, 16222, 3034, 2977, 1010, 2101, 10787, 2000, 1996, 7069, 1022, 1010, 2073, 1996, 3554, 3493, 2439, 2006, 1037, 4771, 12610, 2121, 1011, 3786, 2121, 2114, 2059, 15188, 5612, 1012, 2419, 2011, 6452, 4433, 11214, 15333, 6862, 3946, 1998, 6986, 9530, 2532, 18533, 2239, 1010, 1996, 3554, 3493, 3786, 1996, 9523, 2120, 3410, 3804, 2630, 13664, 3807, 2076, 1996, 2161, 1012, 1996, 3590, 5222, 2020, 102], [101, 2129, 2116, 5222, 2515, 1996, 10289, 8214, 2273, 1005, 1055, 3455, 2136, 2031, 1029, 102, 2528, 1012, 1996, 2230, 1516, 2340, 2136, 5531, 2049, 3180, 2161, 4396, 2193, 2698, 1999, 1996, 2406, 1010, 2007, 1037, 2501, 1997, 2423, 1516, 1019, 1010, 7987, 3240, 1005, 1055, 3587, 3442, 2322, 1011, 2663, 2161, 1010, 1998, 1037, 2117, 1011, 2173, 3926, 1999, 1996, 2502, 2264, 1012, 2076, 1996, 2297, 1011, 2321, 2161, 1010, 1996, 2136, 2253, 3590, 1011, 1020, 1998, 2180, 1996, 16222, 3034, 2977, 1010, 2101, 10787, 2000, 1996, 7069, 1022, 1010, 2073, 1996, 3554, 3493, 2439, 2006, 1037, 4771, 12610, 2121, 1011, 3786, 2121, 2114, 2059, 15188, 5612, 1012, 2419, 2011, 6452, 4433, 11214, 15333, 6862, 3946, 1998, 6986, 9530, 2532, 18533, 2239, 1010, 1996, 3554, 3493, 3786, 1996, 9523, 2120, 3410, 3804, 2630, 13664, 3807, 2076, 1996, 2161, 1012, 1996, 3590, 5222, 2020, 1996, 2087, 2011, 1996, 3554, 3493, 2136, 2144, 5316, 1011, 5641, 1012, 102]]\n",
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n",
      "384\n",
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "print(tokenized_example[\"input_ids\"])\n",
    "print(sequence_ids)\n",
    "print(len(sequence_ids)) # 长度是多少，是截断前？\n",
    "sequence_ids_1 = tokenized_example.sequence_ids(1)\n",
    "print(sequence_ids_1)\n",
    "print(len(sequence_ids_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00d6a210-7515-4881-8b81-696822e3faba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 26\n"
     ]
    }
   ],
   "source": [
    "answers = example[\"answers\"]\n",
    "start_char = answers[\"answer_start\"][0]\n",
    "end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "# 当前span在文本中的起始标记索引。\n",
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "    token_start_index += 1\n",
    "\n",
    "# 当前span在文本中的结束标记索引。\n",
    "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1\n",
    "\n",
    "# 检测答案是否超出span范围（如果超出范围，该特征将以CLS标记索引标记）。\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "    # 将token_start_index和token_end_index移动到答案的两端。\n",
    "    # 注意：如果答案是最后一个单词，我们可以移到最后一个标记之后（边界情况）。\n",
    "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "        token_start_index += 1\n",
    "    start_position = token_start_index - 1\n",
    "    while offsets[token_end_index][1] >= end_char:\n",
    "        token_end_index -= 1\n",
    "    end_position = token_end_index + 1\n",
    "    print(start_position, end_position)\n",
    "else:\n",
    "    print(\"答案不在此特征中。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89888e5-eeff-4e92-a5eb-bf57739e762f",
   "metadata": {},
   "source": [
    "#### 关于填充的策略\n",
    "\n",
    "- 对于没有超过最大长度的文本，填充补齐长度。\n",
    "- 对于需要左侧填充的模型，交换 question 和 context 顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72f93253-cb50-4b0d-b01d-463660d66c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7ce10e-51ae-4738-b27a-4d7a29104b85",
   "metadata": {},
   "source": [
    "### 整合以上所有预处理步骤\n",
    "\n",
    "让我们将所有内容整合到一个函数中，并将其应用到训练集。\n",
    "\n",
    "针对不可回答的情况（上下文过长，答案在另一个特征中），我们为开始和结束位置都设置了cls索引。\n",
    "\n",
    "如果allow_impossible_answers标志为False，我们还可以简单地从训练集中丢弃这些示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca05ea7b-60fb-41bc-8491-3683b391c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # 一些问题的左侧可能有很多空白字符，这对我们没有用，而且会导致上下文的截断失败\n",
    "    # （标记化的问题将占用大量空间）。因此，我们删除左侧的空白字符。\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # 使用截断和填充对我们的示例进行标记化，但保留溢出部分，使用步幅（stride）。\n",
    "    # 当上下文很长时，这会导致一个示例可能提供多个特征，其中每个特征的上下文都与前一个特征的上下文有一些重叠。\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # 由于一个示例可能给我们提供多个特征（如果它具有很长的上下文），我们需要一个从特征到其对应示例的映射。这个键就提供了这个映射关系。\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # 偏移映射将为我们提供从令牌到原始上下文中的字符位置的映射。这将帮助我们计算开始位置和结束位置。\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # 让我们为这些示例进行标记！\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # 我们将使用 CLS 特殊 token 的索引来标记不可能的答案。\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # 获取与该示例对应的序列（以了解上下文和问题是什么）。\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # 一个示例可以提供多个跨度，这是包含此文本跨度的示例的索引。\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # 如果没有给出答案，则将cls_index设置为答案。\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # 答案在文本中的开始和结束字符索引。\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # 当前跨度在文本中的开始令牌索引。\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # 当前跨度在文本中的结束令牌索引。\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # 检测答案是否超出跨度（在这种情况下，该特征的标签将使用CLS索引）。\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # 否则，将token_start_index和token_end_index移到答案的两端。\n",
    "                # 注意：如果答案是最后一个单词（边缘情况），我们可以在最后一个偏移之后继续。\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2cb095-632e-481a-8ebd-7b32dca44118",
   "metadata": {},
   "source": [
    "#### datasets.map 的进阶使用\n",
    "\n",
    "使用 `datasets.map` 方法将 `prepare_train_features` 应用于所有训练、验证和测试数据：\n",
    "\n",
    "- batched: 批量处理数据。\n",
    "- remove_columns: 因为预处理更改了样本的数量，所以在应用它时需要删除旧列。\n",
    "- load_from_cache_file：是否使用datasets库的自动缓存\n",
    "\n",
    "datasets 库针对大规模数据，实现了高效缓存机制，能够自动检测传递给 map 的函数是否已更改（如果预处理函数没有变化，则可以安全地从缓存中加载数据）。如果在调用 map 时设置 `load_from_cache_file=False`，可以强制重新应用预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11f07e4d-2ee7-4bc2-b3a4-9b7fbc2e1d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633f5207851a400e92beb7c0077e67a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(prepare_train_features,\n",
    "                                  batched=True,\n",
    "                                  remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef017e94-242b-4e59-97a9-c463c56b4d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 88524\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 10784\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad22482-eb8a-4e38-92af-4147ab639d8e",
   "metadata": {},
   "source": [
    "## 微调模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a14f0a72-72ed-4b76-b474-02e5948bc18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 由于我们的任务是问答，我们使用 AutoModelForQuestionAnswering 类。(对比 Yelp 评论打分使用的是 AutoModelForSequenceClassification 类）\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "# 警告通知我们正在丢弃一些权重（vocab_transform 和 vocab_layer_norm 层），并随机初始化其他一些权重（pre_classifier 和 classifier 层）。\n",
    "# 在微调模型情况下是绝对正常的，因为我们正在删除用于预训练模型的掩码语言建模任务的头部，并用一个新的头部替换它，\n",
    "# 对于这个新头部，我们没有预训练的权重，所以库会警告我们在用它进行推理之前应该对这个模型进行微调，而这正是我们要做的事情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e376fc53-bf4b-4d9f-bab1-548d50dc6fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练超参数（TrainingArguments）\n",
    "batch_size=64\n",
    "model_dir = f\"models/s_{model_checkpoint}-finetuned-squad\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffb3541b-e60b-4ddf-93b9-14ad378e838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator（数据整理器）\n",
    "# 数据整理器将训练数据整理为批次数据，用于模型训练时的批次处理。本教程使用默认的 default_data_collator。\n",
    "from transformers import default_data_collator\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "424baa51-5784-45ad-9aa3-2558a1945c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化训练器（Trainer）\n",
    "# 为了减少训练时间（需要大量算力支持），我们不在本教程的训练模型过程中计算模型评估指标。\n",
    "# 而是训练完成后，再独立进行模型评估。\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "954532d2-4c3d-402b-9a6c-14b7038486e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4152' max='4152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4152/4152 2:23:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.495900</td>\n",
       "      <td>1.256835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.116000</td>\n",
       "      <td>1.176102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.978800</td>\n",
       "      <td>1.165134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4152, training_loss=1.3058803058543416, metrics={'train_runtime': 8630.3939, 'train_samples_per_second': 30.772, 'train_steps_per_second': 0.481, 'total_flos': 2.602335381127373e+16, 'train_loss': 1.3058803058543416, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ea783db-91b6-4901-a32d-d907da138786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练完成后，第一时间保存模型权重文件。\n",
    "model_to_save = trainer.save_model(model_dir)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f6651-3ddd-4989-8921-5c7826e0c5cd",
   "metadata": {},
   "source": [
    "## 模型评估\n",
    "\n",
    "**评估模型输出需要一些额外的处理：将模型的预测映射回上下文的部分。**\n",
    "\n",
    "模型直接输出的是预测答案的`起始位置`和`结束位置`的**logits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f28784f-bfbe-43a3-8726-ab58f10b7e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'start_positions', 'end_positions'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'start_logits', 'end_logits'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 从评估数据加载器中获取第一个批次的数据\n",
    "for batch in trainer.get_eval_dataloader():\n",
    "    break\n",
    "print(batch.keys()) #输出看下batch的内容\n",
    "\n",
    "# 将批次数据中的每个张量移动到模型所在的设备上（例如，GPU或CPU）。\n",
    "# trainer.args.device 是模型设备的参数。\n",
    "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
    "\n",
    "# 在不进行梯度计算的上下文中，使用模型对批次数据进行前向传递，\n",
    "# 并将输出存储在 output 中。这一步可以节省内存并加快计算速度，\n",
    "# 因为在评估时不需要计算梯度。\n",
    "with torch.no_grad():\n",
    "    output = trainer.model(**batch)\n",
    "\n",
    "# 查看模型输出的键。输出通常是一个字典，包含不同类型的输出，\n",
    "# 如 logits、loss、hidden_states 等。\n",
    "output.keys()\n",
    "# 输出结果：模型的输出是一个类似字典的对象，\n",
    "# 其中包含损失（因为我们提供了标签），以及起始和结束logits。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07973fd5-e8ae-4a09-a6d7-101c67bcf09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 384]), torch.Size([64, 384]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.shape, output.end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74481c2a-569f-4aa4-9a9b-ebdfe58cc582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.5603, -7.4688, -7.7152, -7.7488, -7.6511, -7.3598, -7.5601, -7.5943,\n",
       "        -7.2283, -7.8643, -7.9576, -8.0487, -7.8796, -4.7572, -7.4430, -6.8617,\n",
       "        -6.6888, -5.9366, -5.9683, -6.1733, -6.9719, -6.7137, -7.0810, -6.0526,\n",
       "        -6.0440, -8.0513, -6.0930, -4.7858, -6.8552, -6.7688, -7.8143, -5.9664,\n",
       "        -7.7465, -7.2815, -6.8871, -5.1092, -7.8325, -6.4506,  2.6616,  2.9329,\n",
       "        -4.4743, -4.3986, -5.4108, -1.2398, -5.2374, -1.6214,  7.9821,  3.7006,\n",
       "        -3.5196,  0.4006], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits[0][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "673a7233-9819-4f3a-84d5-60ea47741450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 46,  57,  78,  43, 118, 107,  72,  35, 107,  34,  73,  41,  80,  91,\n",
       "         156,  35,  83,  91,  80,  58,  77,  31,  42,  53,  41,  35,  42,  77,\n",
       "          11,  44,  27, 133,  66,  40,  87,  44,  43,  83, 127,  26,  28,  33,\n",
       "          87, 127,  95,  25,  43, 132,  42,  29,  44,  46,  24,  44,  65,  58,\n",
       "          81,  13,  59,  72,  25,  36,  57,  43], device='cuda:0'),\n",
       " tensor([ 47,  58,  81,  44, 118, 110,  75,  37, 110,  36,  76,  42,  83,  94,\n",
       "         158,  35,  83,  94,  83,  60,  80,  31,  43,  54,  42,  35,  43,  80,\n",
       "          13,  45,  28, 133,  66,  41,  89,  45,  44,  85, 127,  27,  30,  34,\n",
       "          90, 127,  97,  26,  44, 132,  43,  30,  45,  47,  25,  45,  65,  59,\n",
       "          81,  14,  60,  72,  25,  36,  58,  43], device='cuda:0'))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2065ab42-3469-4ec0-906f-95668aebc676",
   "metadata": {},
   "source": [
    "#### 如何从模型输出的位置 logit 组合成答案\n",
    "\n",
    "为了对答案进行分类，\n",
    "1. 将使用通过添加起始和结束logits获得的分数\n",
    "1. 设计一个名为`n_best_size`的超参数，限制不对所有可能的答案进行排序。\n",
    "1. 我们将选择起始和结束logits中的最佳索引，并收集这些预测的所有答案。\n",
    "1. 在检查每一个是否有效后，我们将按照其分数对它们进行排序，并保留最佳的答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d59d8273-1ead-4278-9df4-b082f5dc9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df20de0d-f643-471b-a89e-5eb9b13deb80",
   "metadata": {},
   "source": [
    "```\n",
    "import numpy as np\n",
    "\n",
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "\n",
    "# 获取最佳的起始和结束位置的索引：\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "\n",
    "valid_answers = []\n",
    "\n",
    "# 遍历起始位置和结束位置的索引组合\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        if start_index <= end_index:  # 需要进一步测试以检查答案是否在上下文中\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": \"\"  # TODO: 我们需要找到一种方法来获取与上下文中答案对应的原始子字符串\n",
    "                }\n",
    "            )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5a75e4-eaa2-438a-8a15-0bc420baff45",
   "metadata": {},
   "source": [
    "#### 预处理验证集\n",
    "然后，我们可以根据它们的得分对`valid_answers`进行排序，并仅保留最佳答案。唯一剩下的问题是如何检查给定的跨度是否在上下文中（而不是问题中），以及如何获取其中的文本。为此，我们需要向我们的验证特征添加两个内容：\n",
    "- **example_id**：生成该特征的example的id（因为每个示例可以生成多个特征，如前所示）；\n",
    "- **offset_mapping**: 偏移映射，它将为我们提供从token索引到context中字符位置的映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c58d42a7-d411-4d5b-8571-a1e5a9f07a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # 一些问题的左侧有很多空白，这些空白并不有用且会导致上下文截断失败（分词后的问题会占用很多空间）。\n",
    "    # 因此我们移除这些左侧空白\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # 使用截断和可能的填充对我们的示例进行分词，但使用步长保留溢出的令牌。这导致一个长上下文的示例可能产生\n",
    "    # 几个特征，每个特征的上下文都会稍微与前一个特征的上下文重叠。\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # 由于一个示例在上下文很长时可能会产生几个特征，我们需要一个从特征映射到其对应示例的映射。这个键就是为了这个目的。\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # 我们保留产生这个特征的示例ID，并且会存储偏移映射。\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # 获取与该示例对应的序列（以了解哪些是上下文，哪些是问题）。\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # 一个示例可以产生几个文本段，这里是包含该文本段的示例的索引。\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # 将不属于context的offset_mapping设置为None，以便容易确定一个token位置是否属于上context。\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a85599d7-c6c7-45fe-97d0-67f1a09a7893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6abc5cb572480f8a55cfd2d23f6206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 将prepare_validation_features应用到整个验证集：\n",
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00d4b1f9-f079-48e1-b945-4fd5e889cf43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
       "    num_rows: 10784\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de5da4-c340-4b9b-9954-d4b00c01c578",
   "metadata": {},
   "source": [
    "在 Hugging Face 的 `transformers` 库中，`offset_mapping` 是一个在分词过程中生成的列表，它包含了每个 token 在原始输入文本中的字符位置。这个列表的每个元素是一个元组 `(start_char, end_char)`，表示该 token 在原始输入文本中的起始和结束字符位置。\n",
    "\n",
    "当我们处理问答任务时，输入通常是由`question`和`context`拼接而成的，因此 `offset_mapping` 会包含整个输入序列的字符索引。但在预处理过程中，我们通常会将 `offset_mapping` 的非`context`部分（即`question`部分）的字符索引设置为 `None`，以便于我们在后处理阶段只关注`context`中的字符索引。\n",
    "\n",
    "假设我们有如下输入：\n",
    "\n",
    "- `question`: \"What is AI?\"\n",
    "- `context`: \"Artificial intelligence (AI) is intelligence demonstrated by machines.\"\n",
    "\n",
    "分词后的输入可能是：\n",
    "\n",
    "```plaintext\n",
    "[CLS] What is AI? [SEP] Artificial intelligence (AI) is intelligence demonstrated by machines. [SEP]\n",
    "```\n",
    "\n",
    "对应的 `offset_mapping` 可能是：\n",
    "\n",
    "```python\n",
    "offset_mapping = [\n",
    "    (0, 0), (0, 4), (5, 7), (8, 10), (0, 0),  # question部分\n",
    "    (0, 10), (11, 23), (24, 27), (28, 30), (31, 33), (34, 47), (48, 61), (62, 64), (65, 72), (0, 0)  # context部分\n",
    "]\n",
    "```\n",
    "\n",
    "在预处理阶段，我们通常会将`question`部分的`offset`设置为 `None`：\n",
    "\n",
    "```python\n",
    "offset_mapping = [\n",
    "    None, None, None, None, None,\n",
    "    (0, 10), (11, 23), (24, 27), (28, 30), (31, 33), (34, 47), (48, 61), (62, 64), (65, 72), None\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7550f8a-414f-4eb8-9203-e10b12a2c27b",
   "metadata": {},
   "source": [
    "#### Grab the predictions for all features by using the Trainer.predict method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3012fa94-9315-418d-9c97-55923fbc598f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
       "    num_rows: 10784\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_predictions = trainer.predict(validation_features)\n",
    "validation_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cea10cae-e615-4f1a-89c7-d9981803707f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'56be4db0acb8001400a502ec'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_features[\"input_ids\"][0]\n",
    "validation_features[\"offset_mapping\"][0]\n",
    "validation_features[\"example_id\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7acf1a9d-164f-4bcc-9456-43adf874545e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
       "    num_rows: 10784\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# `Trainer`会隐藏模型不使用的列（在这里是`example_id`和`offset_mapping`，\n",
    "# 我们需要它们进行后处理），所以我们需要将它们重新设置回来：\n",
    "\n",
    "# validation_features.features.keys() 获取当前数据集中所有特征的键。\n",
    "# list(...) 将这些键转换为列表，并传递给 columns 参数，确保所有特征列都包括在内。\n",
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))\n",
    "validation_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d531642f-f9f7-4865-bef3-7c5d887006cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'56be4db0acb8001400a502ec'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_features[\"input_ids\"][0]\n",
    "validation_features[\"offset_mapping\"][0]\n",
    "validation_features[\"example_id\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b704ab-7416-4c8c-9499-557b9ed4dc02",
   "metadata": {},
   "source": [
    "现在，我们可以改进之前的测试：\n",
    "\n",
    "由于在偏移映射中，当它对应于问题的一部分时，我们将其设置为None，因此可以轻松检查答案是否完全在上下文中。我们还可以从考虑中排除非常长的答案（可以调整的超参数）。\n",
    "\n",
    "展开说下具体实现：\n",
    "- 首先从模型输出中获取起始和结束的逻辑值（logits），这些值表明答案在文本中可能开始和结束的位置。\n",
    "- 然后，它使用偏移映射（offset_mapping）来找到这些逻辑值在原始文本中的具体位置。\n",
    "- 接下来，代码遍历可能的开始和结束索引组合，排除那些不在上下文范围内或长度不合适的答案。\n",
    "- 对于有效的答案，它计算出一个分数（基于开始和结束逻辑值的和），并将答案及其分数存储起来。\n",
    "- 最后，它根据分数对答案进行排序，并返回得分最高的几个答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ece64bc2-9868-4488-98db-2aab7935e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99e5bcc7-fc4b-4ca2-9e30-2a54b76323ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 15.650255, 'text': 'Denver Broncos'},\n",
       " {'score': 13.55121,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 11.740786, 'text': 'Carolina Panthers'},\n",
       " {'score': 11.368709, 'text': 'Broncos'},\n",
       " {'score': 10.600998,\n",
       "  'text': 'American Football Conference (AFC) champion Denver Broncos'},\n",
       " {'score': 10.329693,\n",
       "  'text': 'The American Football Conference (AFC) champion Denver Broncos'},\n",
       " {'score': 9.943946, 'text': 'Denver'},\n",
       " {'score': 9.269664,\n",
       "  'text': 'Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 8.66136,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference'},\n",
       " {'score': 8.501953,\n",
       "  'text': 'American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 8.230648,\n",
       "  'text': 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 8.008654,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC)'},\n",
       " {'score': 7.61741,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC'},\n",
       " {'score': 7.5482154,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10'},\n",
       " {'score': 7.1846013,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina'},\n",
       " {'score': 6.4283037, 'text': 'AFC) champion Denver Broncos'},\n",
       " {'score': 6.046736, 'text': 'champion Denver Broncos'},\n",
       " {'score': 5.9696245,\n",
       "  'text': 'the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 5.7377906, 'text': 'Carolina Panthers 24–10'},\n",
       " {'score': 5.7037463, 'text': 'Panthers'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
    "\n",
    "# 第一个特征来自第一个示例。对于更一般的情况，我们需要将example_id匹配到一个示例索引\n",
    "context = datasets[\"validation\"][0][\"context\"]\n",
    "\n",
    "# 收集最佳开始/结束逻辑的索引：\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # 不考虑超出范围的答案，原因是索引超出范围或对应于输入ID的部分不在上下文中。\n",
    "        if (\n",
    "            start_index >= len(offset_mapping)\n",
    "            or end_index >= len(offset_mapping)\n",
    "            or offset_mapping[start_index] is None\n",
    "            or offset_mapping[end_index] is None\n",
    "        ):\n",
    "            continue\n",
    "        # 不考虑长度小于0或大于max_answer_length的答案。\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "            continue\n",
    "        if start_index <= end_index: # 我们需要细化这个测试，以检查答案是否在上下文中\n",
    "            start_char = offset_mapping[start_index][0]\n",
    "            end_char = offset_mapping[end_index][1]\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": context[start_char: end_char]\n",
    "                }\n",
    "            )\n",
    "\n",
    "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "valid_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc46a93-f239-4c27-9bf3-d77d31bd22aa",
   "metadata": {},
   "source": [
    "打印比较模型输出和标准答案（Ground-truth）是否一致:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0946a2e1-6ac5-474e-a50e-701258afaab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       " 'answer_start': [177, 177, 177]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"validation\"][0][\"answers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaf0af4-b198-414b-aafc-59d9d2702ba1",
   "metadata": {},
   "source": [
    "**模型最高概率的输出与标准答案一致**\n",
    "\n",
    "正如上面的代码所示，这在第一个特征上很容易，因为我们知道它来自第一个示例。\n",
    "\n",
    "对于其他特征，我们需要建立一个示例与其对应特征的映射关系。\n",
    "\n",
    "此外，由于一个示例可以生成多个特征，我们需要将由给定示例生成的所有特征中的所有答案汇集在一起，然后选择最佳答案。\n",
    "\n",
    "下面的代码构建了一个示例索引到其对应特征索引的映射关系："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6dd8c12-9bb5-4970-8809-699f13419af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def map_features_to_examples(examples, features):\n",
    "    \"\"\"\n",
    "    将特征映射到示例的函数。\n",
    "\n",
    "    参数:\n",
    "    examples (dict): 包含示例信息的字典，应包含 \"id\" 键。\n",
    "    features (list): 包含特征信息的列表，应包含 \"example_id\" 键。\n",
    "\n",
    "    返回:\n",
    "    dict: 一个从示例索引到其对应特征索引列表的映射字典。\n",
    "    \"\"\"\n",
    "    # 创建从示例 ID 到其索引的映射\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "\n",
    "    # 创建一个默认字典，用于存储每个示例对应的特征索引列表\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "\n",
    "    # 遍历所有特征，将特征索引添加到对应的示例中\n",
    "    for i, feature in enumerate(features):\n",
    "        example_index = example_id_to_index[feature[\"example_id\"]]\n",
    "        features_per_example[example_index].append(i)\n",
    "\n",
    "    return features_per_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be623864-6efe-462b-8c3f-ded7f9bd7946",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = datasets[\"validation\"]\n",
    "features = validation_features\n",
    "features_per_example = map_features_to_examples(examples, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a52563ec-ff75-4a44-9348-91998e3aba07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{250: [250, 251], 251: [252, 253], 252: [254, 255], 253: [256, 257], 254: [258, 259], 255: [260, 261], 256: [262, 263], 257: [264, 265], 258: [266, 267], 259: [268, 269], 260: [270, 271], 261: [272, 273], 262: [274, 275], 263: [276, 277], 264: [278, 279], 265: [280, 281], 266: [282, 283], 267: [284, 285], 268: [286, 287], 269: [288, 289], 1093: [1113, 1114], 1094: [1115, 1116], 1095: [1117, 1118], 2117: [2140, 2141], 2118: [2142, 2143], 2119: [2144, 2145], 2120: [2146, 2147], 2121: [2148, 2149], 2122: [2150, 2151], 2123: [2152, 2153], 2124: [2154, 2155], 2125: [2156, 2157], 2494: [2526, 2527], 2495: [2528, 2529], 2496: [2530, 2531], 2497: [2532, 2533], 2498: [2534, 2535], 4142: [4179, 4180, 4181], 4143: [4182, 4183, 4184], 4144: [4185, 4186], 4145: [4187, 4188, 4189], 4146: [4190, 4191, 4192], 4147: [4193, 4194, 4195], 4148: [4196, 4197, 4198], 4149: [4199, 4200, 4201], 4150: [4202, 4203], 4151: [4204, 4205, 4206], 4180: [4235, 4236], 4181: [4237, 4238], 4182: [4239, 4240], 4183: [4241, 4242], 4184: [4243, 4244], 4185: [4245, 4246], 4186: [4247, 4248], 4187: [4249, 4250], 4188: [4251, 4252], 4189: [4253, 4254], 4190: [4255, 4256], 4195: [4261, 4262], 4196: [4263, 4264], 4197: [4265, 4266], 4198: [4267, 4268], 4203: [4273, 4274], 4204: [4275, 4276], 4205: [4277, 4278], 4206: [4279, 4280], 4207: [4281, 4282], 4255: [4330, 4331], 4256: [4332, 4333], 4257: [4334, 4335], 4258: [4336, 4337], 4259: [4338, 4339], 4260: [4340, 4341, 4342], 4261: [4343, 4344, 4345], 4262: [4346, 4347, 4348], 4263: [4349, 4350, 4351], 4264: [4352, 4353, 4354], 4265: [4355, 4356, 4357], 4266: [4358, 4359, 4360], 4267: [4361, 4362, 4363], 4268: [4364, 4365, 4366], 4269: [4367, 4368, 4369], 4270: [4370, 4371], 4271: [4372, 4373], 4272: [4374, 4375], 4273: [4376, 4377], 4274: [4378, 4379], 4275: [4380, 4381], 4276: [4382, 4383], 4277: [4384, 4385], 4278: [4386, 4387], 4279: [4388, 4389, 4390], 4280: [4391, 4392, 4393], 4281: [4394, 4395, 4396], 4282: [4397, 4398, 4399], 4283: [4400, 4401, 4402], 4288: [4407, 4408], 4289: [4409, 4410], 4290: [4411, 4412], 4291: [4413, 4414], 4292: [4415, 4416], 4297: [4421, 4422, 4423], 4298: [4424, 4425, 4426], 4299: [4427, 4428, 4429], 4300: [4430, 4431, 4432], 4301: [4433, 4434], 4302: [4435, 4436], 4303: [4437, 4438], 4304: [4439, 4440], 4305: [4441, 4442], 4594: [4731, 4732], 4595: [4733, 4734], 4596: [4735, 4736], 4597: [4737, 4738], 4598: [4739, 4740], 4640: [4782, 4783], 4641: [4784, 4785], 4642: [4786, 4787], 4643: [4788, 4789], 4644: [4790, 4791], 4754: [4901, 4902], 4755: [4903, 4904], 4756: [4905, 4906], 4757: [4907, 4908], 4758: [4909, 4910], 4850: [5002, 5003, 5004], 4851: [5005, 5006, 5007], 4852: [5008, 5009, 5010], 4853: [5011, 5012, 5013], 5444: [5604, 5605], 5445: [5606, 5607], 5446: [5608, 5609], 5447: [5610, 5611], 5448: [5612, 5613], 5479: [5644, 5645], 5480: [5646, 5647], 5481: [5648, 5649], 5482: [5650, 5651], 5483: [5652, 5653], 5652: [5822, 5823], 5653: [5824, 5825], 5654: [5826, 5827], 5655: [5828, 5829], 5656: [5830, 5831], 6223: [6398, 6399], 6224: [6400, 6401], 6225: [6402, 6403], 6226: [6404, 6405], 6227: [6406, 6407], 6373: [6553, 6554], 6374: [6555, 6556], 6375: [6557, 6558], 6376: [6559, 6560], 6377: [6561, 6562], 7331: [7516, 7517], 7332: [7518, 7519], 7333: [7520, 7521], 7334: [7522, 7523], 7335: [7524, 7525], 7336: [7526, 7527], 7337: [7528, 7529], 7338: [7530, 7531], 7339: [7532, 7533], 7340: [7534, 7535], 7653: [7848, 7849], 7654: [7850, 7851], 7655: [7852, 7853], 7656: [7854, 7855], 7657: [7856, 7857], 7743: [7943, 7944], 7744: [7945, 7946], 7745: [7947, 7948], 7746: [7949, 7950], 7747: [7951, 7952], 8273: [8478, 8479], 8274: [8480, 8481], 8275: [8482, 8483], 8276: [8484, 8485], 8277: [8486, 8487], 10160: [10370, 10371], 10161: [10372, 10373], 10162: [10374, 10375], 10163: [10376, 10377]}\n"
     ]
    }
   ],
   "source": [
    "print({k: v for k,v in features_per_example.items() if len(v)>1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b751a04-6219-4297-a146-8afdcdafa3e6",
   "metadata": {},
   "source": [
    "当`squad_v2 = True`时，有一定概率出现不可能的答案（impossible answer)。\n",
    "\n",
    "上面的代码仅保留在上下文中的答案，我们还需要获取不可能答案的分数（其起始和结束索引对应于CLS标记的索引）。\n",
    "\n",
    "当一个示例生成多个特征时，我们必须在所有特征中的不可能答案都预测出现不可能答案时（因为一个特征可能之所以能够预测出不可能答案，是因为答案不在它可以访问的上下文部分），这就是为什么一个示例中不可能答案的分数是该示例生成的每个特征中的不可能答案的分数的最小值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "68ab89a0-5a89-41b6-924a-c18d53977ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # 构建一个从示例到其对应特征的映射。\n",
    "    features_per_example = map_features_to_examples(examples, features)\n",
    "\n",
    "    # 我们需要填充的字典。\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # 日志记录。\n",
    "    print(f\"正在后处理 {len(examples)} 个示例的预测，这些预测分散在 {len(features)} 个特征中。\")\n",
    "\n",
    "    # 遍历所有示例！\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # 这些是与当前示例关联的特征的索引。\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # 仅在squad_v2为True时使用。\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # 遍历与当前示例关联的所有特征。\n",
    "        for feature_index in feature_indices:\n",
    "            # 我们获取模型对这个特征的预测。\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # 这将允许我们将logits中的某些位置映射到原始上下文中的文本跨度。\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # 更新最小空预测。\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # 浏览所有的最佳开始和结束logits，为 `n_best_size` 个最佳选择。\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # 不考虑超出范围的答案，原因是索引超出范围或对应于输入ID的部分不在上下文中。\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # 不考虑长度小于0或大于max_answer_length的答案。\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # 在极少数情况下我们没有一个非空预测，我们创建一个假预测以避免失败。\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # 选择我们的最终答案：最佳答案或空答案（仅适用于squad_v2）\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "193da096-1de7-499a-9651-781bb5c8b344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在后处理 10570 个示例的预测，这些预测分散在 10784 个特征中。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e321ddd95d0445e2beae565de96c576a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 在原始结果上应用后处理问答结果：\n",
    "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "61d46d8e-47da-4017-a86d-c3534d12c253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'56be4db0acb8001400a502ec': 'Denver Broncos',\n",
       " '56be4db0acb8001400a502ed': 'Carolina Panthers',\n",
       " '56be4db0acb8001400a502ee': \"Levi's Stadium\",\n",
       " '56be4db0acb8001400a502ef': 'Denver Broncos',\n",
       " '56be4db0acb8001400a502f0': 'gold',\n",
       " '56be8e613aeaaa14008c90d1': '\"golden anniversary\"',\n",
       " '56be8e613aeaaa14008c90d2': 'February 7, 2016',\n",
       " '56be8e613aeaaa14008c90d3': 'American Football Conference',\n",
       " '56bea9923aeaaa14008c91b9': '\"golden anniversary\"',\n",
       " '56bea9923aeaaa14008c91ba': 'American Football Conference'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: final_predictions[k] for i,k in enumerate(final_predictions.keys()) if i < 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e6c53d7b-60f1-4c13-8b76-d412cf2bd77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2655485/2330875496.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")\n",
      "/root/miniconda3/envs/peft/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for squad contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/squad/squad.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6054df58602474590585e83392158cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603019f0b10e4936aba3b1b6696b841b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692cfa42-3632-42ca-b901-fea2a3edc3ca",
   "metadata": {},
   "source": [
    "接下来，我们可以调用上面定义的函数进行评估。\n",
    "\n",
    "只需稍微调整一下预测和标签的格式，因为它期望的是一系列字典而不是一个大字典。\n",
    "\n",
    "在使用`squad_v2`数据集时，我们还需要设置`no_answer_probability`参数（我们在这里将其设置为0.0，因为如果我们选择了答案，我们已经将答案设置为空）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "97f8eedf-e21f-481c-9b8f-4a1e9b3d57c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 74.87228003784296, 'f1': 83.68980445844632}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if squad_v2:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
    "else:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b4b7c0-28cc-402d-9a12-0b0587e886e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
