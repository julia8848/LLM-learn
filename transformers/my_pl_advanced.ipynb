{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f09b6f6-3948-45b3-adf0-d590036272b9",
   "metadata": {},
   "source": [
    "# HF Transformers 核心模块学习：Pipelines 进阶\n",
    "\n",
    "我们已经学习了 Pipeline API 针对各类任务的基本使用。\n",
    "\n",
    "实际上，在 Transformers 库内部实现中，Pipeline 作为管理：`原始文本-输入Token IDs-模型推理-输出概率-生成结果` 的流水线抽象，背后串联了 Transformers 库的核心模块 `Tokenizer`和 `Models`。\n",
    "\n",
    "![](docs/images/pipeline_advanced.png)\n",
    "\n",
    "下面我们开始结合大语言模型（在 Transformers 中也是一种特定任务）学习：\n",
    "\n",
    "- 使用 Pipeline 如何与现代的大语言模型结合，以完成各类下游任务\n",
    "- 使用 Tokenizer 编解码文本\n",
    "- 使用 Models 加载和保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e1700-fa10-4ee2-9605-bec2fef9d9ea",
   "metadata": {},
   "source": [
    "### 使用 GPT-2 实现文本生成\n",
    "\n",
    "![](docs/images/gpt2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "259796dd-60bc-4f91-a590-0309f4c8c3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hugging Face is a community-based open-source platform for machine learning. This is a platform that offers a high degree of self-documentation, user-first, open source design and the creation of open open source hardware. The platform is'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/gpt2\n",
    "from transformers import pipeline\n",
    "\n",
    "prompt = \"Hugging Face is a community-based open-source platform for machine learning.\"\n",
    "generator = pipeline(task=\"text-generation\", model=\"gpt2\")\n",
    "generator(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeab7a38-632b-4ae7-88ca-d8284c711441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"You are very smart. You have mastered the art of taking risks of your own and are prepared to take risks with your friends.\\n\\nThis might sound like an unbelievable quote but really it's a very similar quote from the man.\\n\\nIn\"},\n",
       " {'generated_text': 'You are very smart and that is why I will be the leader of your tribe. You are the best. So, we are going to win and we are going to win again.\"\\n\\nThe first-round bye could be a blow to the'},\n",
       " {'generated_text': \"You are very smart, you can learn something new by following my recipe. If you want to take a step back and get to a greater level of understanding of this process, do yourself a favor and leave a comment. Also, don't be afraid\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置文本生成返回条数\n",
    "prompt = \"You are very smart\"\n",
    "generator = pipeline(task=\"text-generation\", model=\"gpt2\", num_return_sequences=3)\n",
    "generator(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee464510-a55f-4a6a-8931-783614b966fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'You are very smart. He had the good fortune to work in the city after he was killed, and he worked with a lot of his neighbors. This was a very clever city for him. So it was really a place with a good sense of'},\n",
       " {'generated_text': 'You are very smart of you, but I don\\'t give you the benefit of the doubt.\"\\n\\nFang said he had had some initial conversations with his mother over a day and a half, with her telling him she didn\\'t want to talk'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置文本生成返回条数\n",
    "generator(prompt, num_return_sequences=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f08aa4c-9e9d-4e47-80cc-8391f483c368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'You are very smart. If you are not smart at all your job will be'},\n",
       " {'generated_text': \"You are very smart. You're very smart, you must be smart, because\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置文本生成最大长度\n",
    "generator(prompt, num_return_sequences=2, max_length=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ba44f-6f79-4b1e-be4a-b20a7f57c811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3fecae9-cb6f-4938-a042-1861adeac699",
   "metadata": {},
   "source": [
    "### 使用 BERT-Base-Chinese 实现中文补全\n",
    "\n",
    "![](docs/images/bert-base-chinese.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2667507f-0f65-4352-99a7-d78476789ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3297f45f8c4a8787c42d6d9fe7775e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38bded1026d04ac2809441be17dca171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9334a2f8c68b4a4ab714383f0f55ad97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282c19aff5834e9eaffaa3e33251300c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28510bc5626e43a48fb1f71197c1ee00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(task=\"fill-mask\", model=\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da8b30e6-4664-495e-9c9a-c62070f3fa74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9203723669052124,\n",
       "  'token': 679,\n",
       "  'token_str': '不',\n",
       "  'sequence': '人 民 是 不 可 战 胜 的'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"人民是[MASK]可战胜的\"\n",
    "fill_mask(text, top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2ed03dc-77f0-4387-8caa-6b54d2202ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.7596911191940308,\n",
       "  'token': 8043,\n",
       "  'token_str': '？',\n",
       "  'sequence': '美 国 的 首 都 是 ？'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置文本补全的条数\n",
    "text = \"美国的首都是[MASK]\"\n",
    "fill_mask(text, top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33308500-deff-4a62-acc2-c4b91a27da08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9911912083625793,\n",
       "  'token': 3791,\n",
       "  'token_str': '法',\n",
       "  'sequence': '巴 黎 是 法 国 的 首 都 。'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置文本补全的条数\n",
    "text = \"巴黎是[MASK]国的首都。\"\n",
    "fill_mask(text, top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99612a11-0726-433f-baee-e462eae09eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.7596911191940308,\n",
       "  'token': 8043,\n",
       "  'token_str': '？',\n",
       "  'sequence': '美 国 的 首 都 是 ？'},\n",
       " {'score': 0.21126744151115417,\n",
       "  'token': 511,\n",
       "  'token_str': '。',\n",
       "  'sequence': '美 国 的 首 都 是 。'},\n",
       " {'score': 0.026834219694137573,\n",
       "  'token': 8013,\n",
       "  'token_str': '！',\n",
       "  'sequence': '美 国 的 首 都 是 ！'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置文本补全的条数\n",
    "text = \"美国的首都是[MASK]\"\n",
    "fill_mask(text, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79480e4f-e97f-4900-964f-a5f25436d630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.5740261673927307,\n",
       "   'token': 5294,\n",
       "   'token_str': '纽',\n",
       "   'sequence': '[CLS] 美 国 的 首 都 是 纽 [MASK] [MASK] [SEP]'}],\n",
       " [{'score': 0.4926738142967224,\n",
       "   'token': 5276,\n",
       "   'token_str': '约',\n",
       "   'sequence': '[CLS] 美 国 的 首 都 是 [MASK] 约 [MASK] [SEP]'}],\n",
       " [{'score': 0.9353252053260803,\n",
       "   'token': 511,\n",
       "   'token_str': '。',\n",
       "   'sequence': '[CLS] 美 国 的 首 都 是 [MASK] [MASK] 。 [SEP]'}]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"美国的首都是[MASK][MASK][MASK]\"\n",
    "fill_mask(text, top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e9e2bb-0672-40c9-9c08-1b16a8669167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d3cb755-1f51-4eaa-a740-fb0dea3efcf7",
   "metadata": {},
   "source": [
    "## 使用 AutoClass 高效管理 `Tokenizer` 和 `Model`\n",
    "\n",
    "通常，您想要使用的模型（网络架构）可以从您提供给 `from_pretrained()` 方法的预训练模型的名称或路径中推测出来。\n",
    "\n",
    "AutoClasses就是为了帮助用户完成这个工作，以便根据`预训练权重/配置文件/词汇表的名称/路径自动检索相关模型`。\n",
    "\n",
    "比如手动加载`bert-base-chinese`模型以及对应的 `tokenizer` 方法如下：\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e54a270-d5a7-4c37-8081-e83adc6439ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 使用 `from_pretrained` 方法加载指定 Model 和 Tokenizer \n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"bert-base-chinese\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7074fa-6947-4508-84ea-ece6144a480e",
   "metadata": {},
   "source": [
    "#### 使用 BERT Tokenizer 编码文本\n",
    "\n",
    "编码 (Encoding) 过程包含两个步骤：\n",
    "\n",
    "- 分词：使用分词器按某种策略将文本切分为 tokens；\n",
    "- 映射：将 tokens 转化为对应的 token IDs。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b95eddcd-764f-4d2b-9113-f4dca2f298ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['美', '国', '的', '首', '都', '是', '华', '盛', '顿', '特', '区']\n"
     ]
    }
   ],
   "source": [
    "# 第一步：分词\n",
    "sequence = \"美国的首都是华盛顿特区\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8d468ef-0b03-41a6-85c5-c57a620fa58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5401, 1744, 4638, 7674, 6963, 3221, 1290, 4670, 7561, 4294, 1277]\n"
     ]
    }
   ],
   "source": [
    "# 第二步：映射\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bc774d8-911d-4c4b-b8cf-17841940aa98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 5401, 1744, 4638, 7674, 6963, 3221, 1290, 4670, 7561, 4294, 1277, 102]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 Tokenizer.encode 方法端到端处理\n",
    "token_ids_e2e = tokenizer.encode(sequence)\n",
    "token_ids_e2e\n",
    "# 前后新增了 101 和 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db7a0f8b-fa97-4608-960c-88c2c39854b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'美 国 的 首 都 是 华 盛 顿 特 区'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "753bad4d-4dc1-466b-9ab3-06db0de1bcb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 美 国 的 首 都 是 华 盛 顿 特 区 [SEP]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token_ids_e2e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cbb059d-0fbd-439c-8e2b-86ca19a17024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 美 国 的 首 都 是 华 盛 顿 特 区 [SEP] 中 国 的 首 都 是 北 京 [SEP]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_batch = [\"美国的首都是华盛顿特区\", \"中国的首都是北京\"]\n",
    "token_ids_batch = tokenizer.encode(sequence_batch)\n",
    "tokenizer.decode(token_ids_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd490b5-5c0d-4a0e-90ad-ce2c687f2c25",
   "metadata": {},
   "source": [
    "![](docs/images/bert_pretrain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330cf355-89b6-4952-875c-bd167a319dd6",
   "metadata": {},
   "source": [
    "### 实操建议：直接使用 tokenizer.\\_\\_call\\_\\_ 方法完成文本编码 + 特殊编码补全\n",
    "\n",
    "编码后返回结果：\n",
    "\n",
    "```json\n",
    "input_ids: token_ids\n",
    "token_type_ids: token_id 归属的句子编号\n",
    "attention_mask: 指示哪些token需要被关注（注意力机制）\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ed15e4e-5b0e-4b24-8c14-de7fd6589c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 5401, 1744, 4638, 7674, 6963, 3221, 1290, 4670, 7561, 4294, 1277, 102, 704, 1744, 4638, 7674, 6963, 3221, 1266, 776, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "优化下输出结构\n",
      "input_ids: [101, 5401, 1744, 4638, 7674, 6963, 3221, 1290, 4670, 7561, 4294, 1277, 102, 704, 1744, 4638, 7674, 6963, 3221, 1266, 776, 102]\n",
      "token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# 直接使用 tokenizer.__call__ 方法完成文本编码 + 特殊编码补全\n",
    "embedding_batch = tokenizer(\"美国的首都是华盛顿特区\", \"中国的首都是北京\")\n",
    "print(embedding_batch)\n",
    "print(\"\\n优化下输出结构\")\n",
    "for key, value in embedding_batch.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fa1450-e0de-43a7-9b0e-c0eca899e8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62e692d1-936b-4213-8c90-81cc96134b21",
   "metadata": {},
   "source": [
    "### 添加新 Token\n",
    "\n",
    "当出现了词表或嵌入空间中不存在的新Token，需要使用 Tokenizer 将其添加到词表中。 Transformers 库提供了两种不同方法：\n",
    "\n",
    "- add_tokens: 添加常规的正文文本 Token，以追加（append）的方式添加到词表末尾。\n",
    "- add_special_tokens: 添加特殊用途的 Token，优先在已有特殊词表中选择（`bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token`）。如果预定义均不满足，则都添加到`additional_special_tokens`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307b2eb3-23dd-423b-aa2a-4ad3043a97d0",
   "metadata": {},
   "source": [
    "#### 添加常规 Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccd5f20d-9578-4472-b6bc-920d3d27209d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin length: 21128\n",
      "##wi: 10958\n",
      "##滥: 17067\n",
      "##餃: 20675\n",
      "##饥: 20702\n",
      "##门: 20362\n",
      "cafe2017: 8986\n",
      "minkoff: 11018\n",
      "privacy: 8677\n",
      "##煽: 17275\n",
      "轟: 6755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'地支', '天干'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先查看已有词表，确保新添加的 Token 不在词表中：\n",
    "print(f\"origin length: {len(tokenizer.vocab.keys())}\")\n",
    "\n",
    "from itertools import islice\n",
    "# 使用 islice 查看词表部分内容\n",
    "for key, value in islice(tokenizer.vocab.items(), 10):\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "new_tokens = [\"天干\", \"地支\"]\n",
    "new_tokens = set(new_tokens) - set(tokenizer.vocab.keys())\n",
    "new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cac29d77-a2fb-409f-86aa-67fe03146d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将集合作差结果添加到词表中\n",
    "tokenizer.add_tokens(list(new_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee618cea-58ab-43fb-8925-87d8ae3598b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21130"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新增加了2个Token，词表总数由 21128 增加到 21130\n",
    "len(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771bce3b-c1a8-434f-bd1a-256cb57fa5b3",
   "metadata": {},
   "source": [
    "#### 添加特殊Token（审慎操作）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4870665c-9c6d-4409-9c95-6c5ac030bc71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_special_token = {\"sep_token\": \"NEW_SPECIAL_TOKEN\"}\n",
    "tokenizer.add_special_tokens(new_special_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "556bb2c4-2a43-4f8b-b6c2-4b7c1ceca52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21131"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e192b-95f6-4842-91df-30092911b8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "461c3de3-5a4b-4443-9c75-ce5c4c547993",
   "metadata": {},
   "source": [
    "### 使用 `save_pretrained` 方法保存指定 Model 和 Tokenizer \n",
    "\n",
    "借助 `AutoClass` 的设计理念，保存 Model 和 Tokenizer 的方法也相当高效便捷。\n",
    "\n",
    "假设我们对`bert-base-chinese`模型以及对应的 `tokenizer` 做了修改，并更名为`new-bert-base-chinese`，方法如下：\n",
    "\n",
    "```python\n",
    "tokenizer.save_pretrained(\"./models/new-bert-base-chinese\")\n",
    "model.save_pretrained(\"./models/new-bert-base-chinese\")\n",
    "```\n",
    "\n",
    "保存 Tokenizer 会在指定路径下创建以下文件：\n",
    "- tokenizer.json: Tokenizer 元数据文件；\n",
    "- special_tokens_map.json: 特殊字符映射关系配置文件；\n",
    "- tokenizer_config.json: Tokenizer 基础配置文件，存储构建 Tokenizer 需要的参数；\n",
    "- vocab.txt: 词表文件；\n",
    "- added_tokens.json: 单独存放新增 Tokens 的配置文件。\n",
    "\n",
    "保存 Model 会在指定路径下创建以下文件：\n",
    "- config.json：模型配置文件，存储模型结构参数，例如 Transformer 层数、特征空间维度等；\n",
    "- pytorch_model.bin：又称为 state dictionary，存储模型的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40fa4999-4089-49b1-823d-809fe23fef71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/new-bert-base-chinese/tokenizer_config.json',\n",
       " './models/new-bert-base-chinese/special_tokens_map.json',\n",
       " './models/new-bert-base-chinese/vocab.txt',\n",
       " './models/new-bert-base-chinese/added_tokens.json',\n",
       " './models/new-bert-base-chinese/tokenizer.json')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./models/new-bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d03c36c3-862e-434a-a036-0a9ee557fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./models/new-bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15146542-4fb9-413d-850d-16c021b43761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
