{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e67094a5-9255-46e1-bb87-d4afbd87face",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers 微调语言模型-问答任务\n",
    "\n",
    "**注意：微调后的模型仍然是通过提取上下文的子串来回答问题的，而不是生成新的文本。**\n",
    "\n",
    "### 模型执行问答效果示例\n",
    "\n",
    "![Widget inference representing the QA task](docs/images/question_answering.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cca4f8-9264-4b8d-b55b-28804c14b906",
   "metadata": {},
   "source": [
    "## Homework：加载本地保存的模型，进行评估和再训练更高的 F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed566d1b-91dd-4e9b-a574-24c048b537bd",
   "metadata": {},
   "source": [
    "### 关键参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb84d4d-4986-4dcf-982a-b68c0a409597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据你使用的模型和GPU资源情况，调整以下关键参数\n",
    "squad_v2 = True\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 64\n",
    "\n",
    "from_model_dir = f\"models/s_{model_checkpoint}-finetuned-squad\"\n",
    "model_dir = f\"models/{model_checkpoint}-finetuned-squad\"\n",
    "\n",
    "# The maximum length of a feature (question and context)\n",
    "max_length = 384 \n",
    "# The authorized overlap between two part of the context when splitting it is needed.\n",
    "doc_stride = 128 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0c3545-c3fe-4c24-aae7-1378d74d40c1",
   "metadata": {},
   "source": [
    "### 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bdbf6fd-2eb9-4c17-acd5-709878bb826d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 130319\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 11873\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")\n",
    "# The datasets object itself is DatasetDict, which contains one key for the training, validation and test set.\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf3fb10-89ce-414d-b713-7a73f8de7c3a",
   "metadata": {},
   "source": [
    "### 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "277c01c7-8acc-4168-8e76-acd43ab7b41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
    "\n",
    "# 填充的策略\n",
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664b3c6d-4946-4197-9272-60d4da5bcd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # 一些问题的左侧可能有很多空白字符，这对我们没有用，而且会导致上下文的截断失败\n",
    "    # （标记化的问题将占用大量空间）。因此，我们删除左侧的空白字符。\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # 使用截断和填充对我们的示例进行标记化，但保留溢出部分，使用步幅（stride）。\n",
    "    # 当上下文很长时，这会导致一个示例可能提供多个特征，其中每个特征的上下文都与前一个特征的上下文有一些重叠。\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # 由于一个示例可能给我们提供多个特征（如果它具有很长的上下文），我们需要一个从特征到其对应示例的映射。这个键就提供了这个映射关系。\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # 偏移映射将为我们提供从令牌到原始上下文中的字符位置的映射。这将帮助我们计算开始位置和结束位置。\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # 让我们为这些示例进行标记！\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # 我们将使用 CLS 特殊 token 的索引来标记不可能的答案。\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # 获取与该示例对应的序列（以了解上下文和问题是什么）。\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # 一个示例可以提供多个跨度，这是包含此文本跨度的示例的索引。\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # 如果没有给出答案，则将cls_index设置为答案。\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # 答案在文本中的开始和结束字符索引。\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # 当前跨度在文本中的开始令牌索引。\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # 当前跨度在文本中的结束令牌索引。\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # 检测答案是否超出跨度（在这种情况下，该特征的标签将使用CLS索引）。\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # 否则，将token_start_index和token_end_index移到答案的两端。\n",
    "                # 注意：如果答案是最后一个单词（边缘情况），我们可以在最后一个偏移之后继续。\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83864d2a-b49e-448c-965b-6d52ee98a228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb91a617c51455eab7c5278e0b90e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(prepare_train_features,\n",
    "                                  batched=True,\n",
    "                                  remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ff92f-f78d-4079-8c6c-00db1801bc88",
   "metadata": {},
   "source": [
    "### 微调模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "474c988c-7f6c-4397-adfb-94889b65c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(from_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1165a13-ecc6-44c0-8fa9-f620b02d59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练超参数（TrainingArguments）\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1ab6704-634c-47b7-a129-dac72d0d2903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator（数据整理器）\n",
    "# 数据整理器将训练数据整理为批次数据，用于模型训练时的批次处理。本教程使用默认的 default_data_collator。\n",
    "from transformers import default_data_collator\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb51320e-0c6f-426d-8056-e357e3fa472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_trainer = Trainer(\n",
    "    trained_model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a168567-c853-4845-ad09-b9fe38b775f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6177' max='6177' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6177/6177 3:31:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.092000</td>\n",
       "      <td>1.235397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.889600</td>\n",
       "      <td>1.216630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.768800</td>\n",
       "      <td>1.358643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6177, training_loss=0.9445470062991996, metrics={'train_runtime': 12676.3889, 'train_samples_per_second': 31.181, 'train_steps_per_second': 0.487, 'total_flos': 3.873165421863629e+16, 'train_loss': 0.9445470062991996, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e5a7a-66cc-40cc-9fca-bc743b75203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5c440a4-be77-4ae6-b4b4-4098284f19e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练完成后，第一时间保存模型权重文件。\n",
    "model_to_save = trained_trainer.save_model(model_dir)\n",
    "trained_trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592278e-210d-41f8-889c-faf835509776",
   "metadata": {},
   "source": [
    "### 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59e23f40-6abb-4e1d-b711-808bbf051535",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_size = 20\n",
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0770f77b-ea47-4de8-b125-80e9bf19ae04",
   "metadata": {},
   "source": [
    "#### 预处理验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8011b070-a255-46e2-9c69-fe44c54d7130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # 一些问题的左侧有很多空白，这些空白并不有用且会导致上下文截断失败（分词后的问题会占用很多空间）。\n",
    "    # 因此我们移除这些左侧空白\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # 使用截断和可能的填充对我们的示例进行分词，但使用步长保留溢出的令牌。这导致一个长上下文的示例可能产生\n",
    "    # 几个特征，每个特征的上下文都会稍微与前一个特征的上下文重叠。\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # 由于一个示例在上下文很长时可能会产生几个特征，我们需要一个从特征映射到其对应示例的映射。这个键就是为了这个目的。\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # 我们保留产生这个特征的示例ID，并且会存储偏移映射。\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # 获取与该示例对应的序列（以了解哪些是上下文，哪些是问题）。\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # 一个示例可以产生几个文本段，这里是包含该文本段的示例的索引。\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # 将不属于context的offset_mapping设置为None，以便容易确定一个token位置是否属于上context。\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "178ce301-b46c-4f95-9752-4239b707d43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1fc5901ff0412cba13a793f3d1b9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 将prepare_validation_features应用到整个验证集：\n",
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eac17b-5cd8-4f0d-a720-cf8f39a879fe",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26dd648f-aeb4-411c-bd45-05c2bf3ab31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_predictions = trained_trainer.predict(validation_features)\n",
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255b2384-2f16-4a0f-bccc-a684c6b2e75b",
   "metadata": {},
   "source": [
    "#### 后处理预测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8de08aa2-febe-4da8-9cdd-059d291a5990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def map_features_to_examples(examples, features):\n",
    "    # 创建从示例 ID 到其索引的映射\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "\n",
    "    # 创建一个默认字典，用于存储每个示例对应的特征索引列表\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "\n",
    "    # 遍历所有特征，将特征索引添加到对应的示例中\n",
    "    for i, feature in enumerate(features):\n",
    "        example_index = example_id_to_index[feature[\"example_id\"]]\n",
    "        features_per_example[example_index].append(i)\n",
    "\n",
    "    return features_per_example\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # 构建一个从示例到其对应特征的映射。\n",
    "    features_per_example = map_features_to_examples(examples, features)\n",
    "\n",
    "    # 我们需要填充的字典。\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # 日志记录。\n",
    "    print(f\"正在后处理 {len(examples)} 个示例的预测，这些预测分散在 {len(features)} 个特征中。\")\n",
    "\n",
    "    # 遍历所有示例！\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # 这些是与当前示例关联的特征的索引。\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # 仅在squad_v2为True时使用。\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # 遍历与当前示例关联的所有特征。\n",
    "        for feature_index in feature_indices:\n",
    "            # 我们获取模型对这个特征的预测。\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # 这将允许我们将logits中的某些位置映射到原始上下文中的文本跨度。\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # 更新最小空预测。\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # 浏览所有的最佳开始和结束logits，为 `n_best_size` 个最佳选择。\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # 不考虑超出范围的答案，原因是索引超出范围或对应于输入ID的部分不在上下文中。\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # 不考虑长度小于0或大于max_answer_length的答案。\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # 在极少数情况下我们没有一个非空预测，我们创建一个假预测以避免失败。\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # 选择我们的最终答案：最佳答案或空答案（仅适用于squad_v2）\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78dab298-e6f9-45f0-a52d-e78182a5b142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在后处理 11873 个示例的预测，这些预测分散在 12134 个特征中。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76b9fcd17ac40e0b3153abb00ac6608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e0198d-1374-448b-b3c8-9b8d017a6e8b",
   "metadata": {},
   "source": [
    "#### 加载评估指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a16eac8-edee-410d-b7c6-ea1e51665f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3193443/2330875496.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")\n",
      "/root/miniconda3/envs/peft/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for squad_v2 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/squad_v2/squad_v2.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da1574f-e7d9-4d3a-b741-ae12028be76f",
   "metadata": {},
   "source": [
    "#### 计算评估值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "140a0948-f4a7-4a2e-9e85-be4b490e4f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact': 62.73056514781437,\n",
       " 'f1': 66.35280746076715,\n",
       " 'total': 11873,\n",
       " 'HasAns_exact': 66.75101214574899,\n",
       " 'HasAns_f1': 74.0058844436042,\n",
       " 'HasAns_total': 5928,\n",
       " 'NoAns_exact': 58.72161480235492,\n",
       " 'NoAns_f1': 58.72161480235492,\n",
       " 'NoAns_total': 5945,\n",
       " 'best_exact': 62.73056514781437,\n",
       " 'best_exact_thresh': 0.0,\n",
       " 'best_f1': 66.35280746076728,\n",
       " 'best_f1_thresh': 0.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if squad_v2:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
    "else:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe63ad5-b2cf-4215-9337-ddbbb04d1abf",
   "metadata": {},
   "source": [
    "第三个epoch后，Validation Loss突然上升，最终评估的F1值下降了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0872e4d-d601-4190-9f9d-d99ecd2bdedf",
   "metadata": {},
   "source": [
    "灾难性遗忘（Catastrophic Forgetting）是深度学习中一个常见的问题，尤其是在顺序学习（sequential learning）或连续学习（continual learning）中表现得尤为明显。它指的是模型在学习新任务时，会快速遗忘之前学过的任务。这在处理多任务学习或在线学习时尤其需要注意。以下是对灾难性遗忘的详细解释以及一些常见的解决方案：\n",
    "\n",
    "### 灾难性遗忘的解释\n",
    "\n",
    "当模型在没有特定策略的情况下按顺序学习不同的任务时，模型的参数会朝着优化新任务的方向更新，可能会破坏之前任务所需的参数设置。结果是模型在新任务上的表现可能会很好，但在先前任务上的表现会显著下降。\n",
    "\n",
    "### 解决灾难性遗忘的方法\n",
    "\n",
    "1. **正则化方法**\n",
    "   - **Elastic Weight Consolidation (EWC)**: 在训练新任务时，通过增加一个正则项来惩罚与之前任务重要参数的偏离，从而保留以前任务的重要知识。\n",
    "   - **Synaptic Intelligence (SI)**: 类似于 EWC，但它通过追踪参数在不同任务中的重要性并在损失函数中添加正则项来保留重要参数。\n",
    "\n",
    "2. **回放方法**\n",
    "   - **经验回放（Experience Replay）**: 将先前任务的样本存储起来，在训练新任务时与新任务样本一起训练模型。这可以是直接回放样本（如贪婪回放）或生成对抗网络生成的样本。\n",
    "   - **渐进神经网络（Progressive Neural Networks）**: 为每个新任务增加新的专用神经网络模块，原始模块的参数冻结，不再更新，但通过侧链连接共享信息。\n",
    "\n",
    "3. **参数隔离方法**\n",
    "   - **固定子网络（Fixed Subnetworks）**: 为每个任务分配不同的参数子集，并确保这些子集在后续任务中不会被修改。\n",
    "   - **掩码网络（Masked Networks）**: 使用掩码来控制不同任务激活不同的参数子集，从而减少不同任务间的参数冲突。\n",
    "\n",
    "4. **模型扩展方法**\n",
    "   - **动态扩展神经网络（Dynamic Expandable Networks）**: 根据新任务的需要动态增加网络结构和参数，以适应新的任务。\n",
    "\n",
    "### 代码示例：使用 EWC 解决灾难性遗忘\n",
    "\n",
    "以下是使用 PyTorch 实现 EWC 方法的简化示例：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class EWC:\n",
    "    def __init__(self, model, dataloader, lambda_ewc):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.lambda_ewc = lambda_ewc\n",
    "        self.params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
    "        self._means = {}\n",
    "        self._precision_matrices = self._diag_fisher()\n",
    "        for n, p in self.params.items():\n",
    "            self._means[n] = p.clone().detach()\n",
    "\n",
    "    def _diag_fisher(self):\n",
    "        precision_matrices = {n: torch.zeros_like(p) for n, p in self.params.items()}\n",
    "        self.model.eval()\n",
    "        for input, target in self.dataloader:\n",
    "            self.model.zero_grad()\n",
    "            output = self.model(input)\n",
    "            loss = nn.CrossEntropyLoss()(output, target)\n",
    "            loss.backward()\n",
    "            for n, p in self.params.items():\n",
    "                precision_matrices[n].data += p.grad.data ** 2 / len(self.dataloader)\n",
    "        precision_matrices = {n: p for n, p in precision_matrices.items()}\n",
    "        return precision_matrices\n",
    "\n",
    "    def penalty(self):\n",
    "        loss = 0\n",
    "        for n, p in self.model.named_parameters():\n",
    "            if n in self._means:\n",
    "                _loss = self._precision_matrices[n] * (p - self._means[n]) ** 2\n",
    "                loss += _loss.sum()\n",
    "        return loss\n",
    "\n",
    "    def __call__(self, model_output, target):\n",
    "        loss = nn.CrossEntropyLoss()(model_output, target)\n",
    "        ewc_loss = self.penalty()\n",
    "        return loss + self.lambda_ewc * ewc_loss\n",
    "\n",
    "# 假设我们有一个预训练的 model 和 dataloader\n",
    "model = ...  # Your model\n",
    "dataloader = ...  # Your dataloader for the old task\n",
    "lambda_ewc = 0.4  # Regularization strength\n",
    "\n",
    "# 初始化 EWC 实例\n",
    "ewc = EWC(model, dataloader, lambda_ewc)\n",
    "\n",
    "# 在新任务上训练\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "for input, target in new_task_dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input)\n",
    "    loss = ewc(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "### 结论\n",
    "\n",
    "灾难性遗忘是连续学习中的一个重要问题，但通过使用正则化方法、回放方法、参数隔离方法以及模型扩展方法，可以有效地缓解这一问题。选择合适的方法取决于具体的应用场景和计算资源限制。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1a246a-220c-4099-9b64-880def681516",
   "metadata": {},
   "source": [
    "**这是在squad数据集的训练结果上，使用squad v2再训练的，如果两次都用squad v2会不会好点？homework4再试试**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65abf14d-808e-45f9-92d5-91d1a6febc48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
