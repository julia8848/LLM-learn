{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b49b14-8696-435d-bb4d-33bfb0b96139",
   "metadata": {},
   "source": [
    "# HF Transformers 核心模块学习：Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d52cf4d-2d0e-4553-a3c4-ffe0f83a98d6",
   "metadata": {},
   "source": [
    "## transformers 自定义模型下载的路径\n",
    "指定预先下载好的模型的路径，transformers会从该路径加载模型\n",
    "```python\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/mnt/new_volume/hf'\n",
    "os.environ['HF_HUB_CACHE'] = '/mnt/new_volume/hf/hub'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f963b22f-1909-4b40-b2c0-0f3f8852c690",
   "metadata": {},
   "source": [
    "## pipeline API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220badaa-389e-4326-9eac-126e0df9078d",
   "metadata": {},
   "source": [
    "pipeline下载的模型默认的本地缓存目录：`~/.cache/huggingface/hub`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12c96fb6-3e29-474c-83b9-025bc29e5ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d9e56c-8654-44fe-822e-ad9562efe87d",
   "metadata": {},
   "source": [
    "Solve Error: \n",
    "```\n",
    "/root/miniconda3/envs/peft/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
    "  from .autonotebook import tqdm as notebook_tqdm\n",
    "```\n",
    "Solution:\n",
    "This warning message indicates that the `tqdm` library, which is used for showing progress bars, is trying to use `IProgress` for Jupyter notebooks but can't find it. This usually means that `ipywidgets`, which `IProgress` is part of, is not installed or needs to be updated.\n",
    "\n",
    "To resolve this, you can install or update `ipywidgets` in your environment. Here’s how you can do it:\n",
    "\n",
    "1. **Install ipywidgets:**\n",
    "\n",
    "   ```bash\n",
    "   conda install ipywidgets\n",
    "   ```\n",
    "\n",
    "2. **Update Jupyterlab and ipywidgets:**\n",
    "\n",
    "   ```bash\n",
    "   conda update jupyterlab ipywidgets\n",
    "   ```\n",
    "\n",
    "After running these commands, restart your Jupyter notebook server and try running your code again. This should resolve the warning and allow `tqdm` to show progress bars in your Jupyter notebook.\n",
    "**Restart Jupyterlab**   \n",
    "1. Find the PID of JupyterLab:\n",
    "```bash\n",
    "ps aux | grep jupyter-lab\n",
    "```\n",
    "2. Kill the JupyterLab process:\n",
    "```bash\n",
    "kill <PID>\n",
    "```\n",
    "3. Restart JupyterLab\n",
    "```bash\n",
    "nohup jupyter lab --port=8000 --NotebookApp.token='your_password' --notebook-dir=./ &\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a9153-3fbe-42e0-b89c-84fb3c93e704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "742ec413-d220-49b3-941b-fcbd49a646e3",
   "metadata": {},
   "source": [
    "## Text Classification \n",
    "Text classification(文本分类)与任何模态中的分类任务一样，文本分类将一个文本序列（可以是句子级别、段落或者整篇文章）标记为预定义的类别集合之一。文本分类有许多实际应用，其中包括：\n",
    "\n",
    "- **情感分析**：根据某种极性（如积极或消极）对文本进行标记，以在政治、金融和市场等领域支持决策制定。\n",
    "- **内容分类**：根据某个主题对文本进行标记，以帮助组织和过滤新闻和社交媒体信息流中的信息（天气、体育、金融等）。\n",
    "\n",
    "**Default**: distilbert-base-uncased-finetuned-sst-2-english  \n",
    "**模型主页**：https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f934252d-5e5f-4cb7-af7a-40d9c7d9148d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/root/miniconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e4d8123-98d7-4208-b0d7-8185203257bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.8957212567329407}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"今儿上海可真冷啊\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ef0575-9db9-444c-896d-f2a03e31b275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9238728880882263}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"我觉得这家店蒜泥白肉的味道一般\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "395ebf49-a834-43a8-8651-eddef0c8d1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.8578686118125916}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"你学东西真的好快，理论课一讲就明白了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02167f00-888e-4e7d-9083-9249201e5941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9961802959442139}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"You learn things really quickly. You understand the theory class as soon as it is taught.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4a8a40d-ea0d-4ccc-b674-4da151bea7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9995032548904419}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Today Shanghai is really cold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "166250e5-2c98-495a-8217-19e5fcc1d4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9995032548904419},\n",
       " {'label': 'NEGATIVE', 'score': 0.9984821677207947},\n",
       " {'label': 'POSITIVE', 'score': 0.9961802959442139}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = [\n",
    "    \"Today Shanghai is really cold.\",\n",
    "    \"I think the taste of the garlic mashed pork in this store is average.\",\n",
    "    \"You learn things really quickly. You understand the theory class as soon as it is taught.\"\n",
    "]\n",
    "\n",
    "pipe(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "56a478b5-3407-4f4b-bd2f-ee70c89c73ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20635e2437894d38a74fdbd8c1001454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690feaded3694514bcf7d40efe0f9456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/409M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d84e3540f44b609d7ce4a94f070a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/315 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b1e5185b9f40c1a3f299bb99bff3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538db8ca1a534ee0b52d625214eb3a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/439k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf0ac93936a4a6e9f6103507d0937dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.9998294115066528},\n",
       " {'label': 'neutral', 'score': 0.9999804496765137},\n",
       " {'label': 'neutral', 'score': 0.9999808073043823},\n",
       " {'label': 'neutral', 'score': 0.9999808073043823},\n",
       " {'label': 'positive', 'score': 0.7147849202156067},\n",
       " {'label': 'neutral', 'score': 0.9999797344207764}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english\n",
    "pipe = pipeline(\"sentiment-analysis\")\n",
    "text_list = [\n",
    "    \"今儿上海可真冷啊\",\n",
    "    \"Today Shanghai is really cold.\",\n",
    "    \"我觉得这家店蒜泥白肉的味道一般\",\n",
    "    \"I think the taste of the garlic mashed pork in this store is average.\",\n",
    "    \"你学东西真的好快，理论课一讲就明白了\",\n",
    "    \"You learn things really quickly. You understand the theory class as soon as it is taught.\"\n",
    "]\n",
    "\n",
    "pipe(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "184feda6-7327-4f95-9d4b-86b2560f62ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20635e2437894d38a74fdbd8c1001454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690feaded3694514bcf7d40efe0f9456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/409M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d84e3540f44b609d7ce4a94f070a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/315 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b1e5185b9f40c1a3f299bb99bff3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538db8ca1a534ee0b52d625214eb3a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/439k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf0ac93936a4a6e9f6103507d0937dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.9998294115066528},\n",
       " {'label': 'neutral', 'score': 0.9999804496765137},\n",
       " {'label': 'neutral', 'score': 0.9999808073043823},\n",
       " {'label': 'neutral', 'score': 0.9999808073043823},\n",
       " {'label': 'positive', 'score': 0.7147849202156067},\n",
       " {'label': 'neutral', 'score': 0.9999797344207764}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/bardsai/finance-sentiment-zh-base\n",
    "pipe = pipeline(\"sentiment-analysis\", model=\"bardsai/finance-sentiment-zh-base\")\n",
    "text_list = [\n",
    "    \"今儿上海可真冷啊\",\n",
    "    \"Today Shanghai is really cold.\",\n",
    "    \"我觉得这家店蒜泥白肉的味道一般\",\n",
    "    \"I think the taste of the garlic mashed pork in this store is average.\",\n",
    "    \"你学东西真的好快，理论课一讲就明白了\",\n",
    "    \"You learn things really quickly. You understand the theory class as soon as it is taught.\"\n",
    "]\n",
    "\n",
    "pipe(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc4967f-516b-4b2f-a2e0-2626fce66d98",
   "metadata": {},
   "source": [
    "## Natural Language Processing(NLP)\n",
    "\n",
    "**NLP**(自然语言处理)任务是最常见的任务类型之一，因为文本是我们进行交流的一种自然方式。要将文本转换为模型可识别的格式，需要对其进行分词。这意味着将一系列文本划分为单独的单词或子词（标记），然后将这些标记转换为数字。结果就是，您可以将一系列文本表示为一系列数字，并且一旦您拥有了一系列数字，它就可以输入到模型中来解决各种NLP任务！\n",
    "\n",
    "上面演示的 文本分类任务，以及接下来的标记、问答等任务都属于 NLP 范畴。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54190e6e-9412-4013-b7ae-be4bff79634f",
   "metadata": {},
   "source": [
    "### Token Classification\n",
    "\n",
    "在任何NLP任务中，文本都经过预处理，将文本序列分成单个单词或子词。这些被称为tokens。\n",
    "\n",
    "**Token Classification**（Token分类）将每个token分配一个来自预定义类别集的标签。\n",
    "\n",
    "两种常见的 Token 分类是：\n",
    "\n",
    "- 命名实体识别（NER）：根据实体类别（如组织、人员、位置或日期）对token进行标记。NER在生物医学设置中特别受欢迎，可以标记基因、蛋白质和药物名称。\n",
    "- 词性标注（POS）：根据其词性（如名词、动词或形容词）对标记进行标记。POS对于帮助翻译系统了解两个相同的单词如何在语法上不同很有用（作为名词的银行与作为动词的银行）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2b16842e-ea2b-4aaa-a527-c2c672a275ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'I-ORG', 'score': 0.9968, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}\n",
      "{'entity': 'I-ORG', 'score': 0.9293, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}\n",
      "{'entity': 'I-ORG', 'score': 0.9763, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}\n",
      "{'entity': 'I-MISC', 'score': 0.9983, 'index': 6, 'word': 'French', 'start': 18, 'end': 24}\n",
      "{'entity': 'I-LOC', 'score': 0.999, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}\n",
      "{'entity': 'I-LOC', 'score': 0.9987, 'index': 11, 'word': 'York', 'start': 46, 'end': 50}\n",
      "{'entity': 'I-LOC', 'score': 0.9992, 'index': 12, 'word': 'City', 'start': 51, 'end': 55}\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english\n",
    "token_classifier_default = pipeline(task=\"ner\")\n",
    "preds = token_classifier_default(\"Hugging Face is a French company based in New York City.\")\n",
    "preds = [\n",
    "    {\n",
    "        \"entity\": pred[\"entity\"],\n",
    "        \"score\": round(pred[\"score\"], 4),\n",
    "        \"index\": pred[\"index\"],\n",
    "        \"word\": pred[\"word\"],\n",
    "        \"start\": pred[\"start\"],\n",
    "        \"end\": pred[\"end\"],\n",
    "    }\n",
    "    for pred in preds\n",
    "]\n",
    "print(*preds, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "896694a2-d662-4e78-898e-5e3a5f15bf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'ORG',\n",
       "  'score': 0.9674639,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 0,\n",
       "  'end': 12},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.99828726,\n",
       "  'word': 'French',\n",
       "  'start': 18,\n",
       "  'end': 24},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99896103,\n",
       "  'word': 'New York City',\n",
       "  'start': 42,\n",
       "  'end': 55}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english\n",
    "token_classifier_default = pipeline(task=\"ner\", aggregation_strategy=\"simple\")\n",
    "token_classifier_default(\"Hugging Face is a French company based in New York City.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "603c7c08-f511-4c7f-93d2-a6ea4f8412bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'I-ORG', 'score': 0.9664, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}\n",
      "{'entity': 'B-MISC', 'score': 0.9873, 'index': 6, 'word': 'French', 'start': 18, 'end': 24}\n",
      "{'entity': 'B-LOC', 'score': 0.9959, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}\n",
      "{'entity': 'I-LOC', 'score': 0.9959, 'index': 11, 'word': 'York', 'start': 46, 'end': 50}\n",
      "{'entity': 'I-LOC', 'score': 0.9957, 'index': 12, 'word': 'City', 'start': 51, 'end': 55}\n"
     ]
    }
   ],
   "source": [
    "token_classifier = pipeline(task=\"ner\", model=\"dslim/distilbert-NER\")\n",
    "preds = token_classifier(\"Hugging Face is a French company based in New York City.\")\n",
    "preds = [\n",
    "    {\n",
    "        \"entity\": pred[\"entity\"],\n",
    "        \"score\": round(pred[\"score\"], 4),\n",
    "        \"index\": pred[\"index\"],\n",
    "        \"word\": pred[\"word\"],\n",
    "        \"start\": pred[\"start\"],\n",
    "        \"end\": pred[\"end\"],\n",
    "    }\n",
    "    for pred in preds\n",
    "]\n",
    "print(*preds, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e1bde65f-5d5f-4ccd-9997-68c2e8ebaf8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'ORG',\n",
       "  'score': 0.9663961,\n",
       "  'word': 'Face',\n",
       "  'start': 8,\n",
       "  'end': 12},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.98729867,\n",
       "  'word': 'French',\n",
       "  'start': 18,\n",
       "  'end': 24},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9958365,\n",
       "  'word': 'New York City',\n",
       "  'start': 42,\n",
       "  'end': 55}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://huggingface.co/dslim/distilbert-NER\n",
    "token_classifier = pipeline(task=\"ner\", model=\"dslim/distilbert-NER\", aggregation_strategy=\"simple\")\n",
    "token_classifier(\"Hugging Face is a French company based in New York City.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0c1f12-ead5-45db-9097-cb8b949f37e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3172a11f-cd48-4abc-9541-ea1aea0f229b",
   "metadata": {},
   "source": [
    "### Question Answering\n",
    "\n",
    "**Question Answering**(问答)是另一个token-level的任务，返回一个问题的答案，有时带有上下文（开放领域），有时不带上下文（封闭领域）。每当我们向虚拟助手提出问题时，例如询问一家餐厅是否营业，就会发生这种情况。它还可以提供客户或技术支持，并帮助搜索引擎检索您要求的相关信息。\n",
    "\n",
    "有两种常见的问答类型：\n",
    "\n",
    "- 提取式：给定一个问题和一些上下文，模型必须从上下文中提取出一段文字作为答案\n",
    "- 生成式：给定一个问题和一些上下文，答案是根据上下文生成的；这种方法由`Text2TextGenerationPipeline`处理，而不是下面展示的`QuestionAnsweringPipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f2fcb90-dc61-458e-a9cf-b632f1b94fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa(question_answerer, question, context):\n",
    "    preds = question_answerer(\n",
    "        question=question,\n",
    "        context=context,\n",
    "    )\n",
    "    print(\n",
    "        f\"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95eab39c-af1f-4d43-b9d7-aa9482e1d04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9327, start: 30, end: 54, answer: huggingface/transformers\n",
      "score: 0.9458, start: 115, end: 122, answer: Beijing\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/distilbert-base-cased-distilled-squad\n",
    "question_answerer=pipeline(task=\"question-answering\")\n",
    "qa(\n",
    "    question_answerer=question_answerer,\n",
    "    question=\"What is the name of the repository?\",\n",
    "    context=\"The name of the repository is huggingface/transformers\"\n",
    ")\n",
    "qa(\n",
    "    question_answerer=question_answerer,\n",
    "    question=\"What is the capital of China?\",\n",
    "    context=\"On 1 October 1949, CCP Chairman Mao Zedong formally proclaimed the People's Republic of China in Tiananmen Square, Beijing.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "557192ab-ccb8-4e1b-8f97-35f3959fd31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6375e06c6ac9492185081e98b9a73c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b678a6bc4704f55b91e9c666210c132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0d9b42dfd94745a9f8718526d540f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31216bff50a64a378b1ccbbfa01978a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552d27645c104d2bac51e870106c3a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.8276, start: 30, end: 54, answer: huggingface/transformers\n",
      "score: 0.9925, start: 115, end: 122, answer: Beijing\n"
     ]
    }
   ],
   "source": [
    "question_answerer=pipeline(task=\"question-answering\", model=\"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "qa(\n",
    "    question_answerer=question_answerer,\n",
    "    question=\"What is the name of the repository?\",\n",
    "    context=\"The name of the repository is huggingface/transformers\"\n",
    ")\n",
    "qa(\n",
    "    question_answerer=question_answerer,\n",
    "    question=\"What is the capital of China?\",\n",
    "    context=\"On 1 October 1949, CCP Chairman Mao Zedong formally proclaimed the People's Republic of China in Tiananmen Square, Beijing.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b31aa7a-33a4-44af-8d1d-6dbbad08de7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4925375-3c0b-483a-ae26-1d63c94a8040",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "\n",
    "**Summarization**(文本摘要）从较长的文本中创建一个较短的版本，同时尽可能保留原始文档的大部分含义。摘要是一个序列到序列的任务；它输出比输入更短的文本序列。有许多长篇文档可以进行摘要，以帮助读者快速了解主要要点。法案、法律和财务文件、专利和科学论文等文档可以摘要，以节省读者的时间并作为阅读辅助工具。\n",
    "\n",
    "与问答类似，摘要有两种类型：\n",
    "\n",
    "- 提取式：从原始文本中识别和提取最重要的句子\n",
    "- 生成式：从原始文本中生成目标摘要（可能包括输入文件中没有的新单词）；`SummarizationPipeline`使用生成式方法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2b001b72-c30c-446c-84a8-a137eb6d5ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'large language models (LLMs) are very large deep learning models pre-trained on vast amounts of data . transformers are capable of unsupervised training, although a more precise explanation is that they perform self-learning . data scientists can use GPUs for training transformer-based LLMs, significantly reducing the training time .'}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/t5-base\n",
    "summarizer = pipeline(task=\"summarization\",\n",
    "                      model=\"t5-base\",\n",
    "                      min_length=8,\n",
    "                      max_length=100\n",
    ")\n",
    "summarizer(\n",
    "    '''\n",
    "    Large language models (LLM) are very large deep learning models that are pre-trained on vast amounts of data. \n",
    "    The underlying transformer is a set of neural networks that consist of an encoder and a decoder with self-attention capabilities. \n",
    "    The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it.\n",
    "    Transformer LLMs are capable of unsupervised training, although a more precise explanation is that transformers perform self-learning. \n",
    "    It is through this process that transformers learn to understand basic grammar, languages, and knowledge.\n",
    "    Unlike earlier recurrent neural networks (RNN) that sequentially process inputs, transformers process entire sequences in parallel. \n",
    "    This allows the data scientists to use GPUs for training transformer-based LLMs, significantly reducing the training time.\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8372fb05-23aa-4a9c-b401-fcdce5607322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Large language models (LLM) are very large deep learning models that are pre-trained on vast amounts of data . The underlying transformer is a set of neural networks that consist of an encoder and a decoder with self-attention capabilities . Unlike earlier recurrent neural networks (RNN) that sequentially process inputs, transformers process entire sequences in parallel .'}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/sshleifer/distilbart-cnn-12-6\n",
    "summarizer = pipeline(task=\"summarization\",\n",
    "                      min_length=8,\n",
    "                      max_length=100\n",
    ")\n",
    "summarizer(\n",
    "    '''\n",
    "    Large language models (LLM) are very large deep learning models that are pre-trained on vast amounts of data. \n",
    "    The underlying transformer is a set of neural networks that consist of an encoder and a decoder with self-attention capabilities. \n",
    "    The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it.\n",
    "    Transformer LLMs are capable of unsupervised training, although a more precise explanation is that transformers perform self-learning. \n",
    "    It is through this process that transformers learn to understand basic grammar, languages, and knowledge.\n",
    "    Unlike earlier recurrent neural networks (RNN) that sequentially process inputs, transformers process entire sequences in parallel. \n",
    "    This allows the data scientists to use GPUs for training transformer-based LLMs, significantly reducing the training time.\n",
    "    '''\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c462ce0-74b5-4747-82cd-fcdb1805e8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b632edf2-be5d-42d3-ab1e-d3fe45e6e776",
   "metadata": {},
   "source": [
    "## Audio 音频处理任务\n",
    "\n",
    "音频和语音处理任务与其他模态略有不同，主要是因为音频作为输入是一个连续的信号。与文本不同，原始音频波形不能像句子可以被划分为单词那样被整齐地分割成离散的块。为了解决这个问题，通常在固定的时间间隔内对原始音频信号进行采样。如果在每个时间间隔内采样更多样本，采样率就会更高，音频更接近原始音频源。\n",
    "\n",
    "以前的方法是预处理音频以从中提取有用的特征。现在更常见的做法是直接将原始音频波形输入到特征编码器中，以提取音频表示。这样可以简化预处理步骤，并允许模型学习最重要的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d124fb-34b7-4975-b91f-44310965b22d",
   "metadata": {},
   "source": [
    "### Audio classification\n",
    "\n",
    "**Audio classification**(音频分类)是一项将音频数据从预定义的类别集合中进行标记的任务。这是一个广泛的类别，具有许多具体的应用，其中一些包括：\n",
    "\n",
    "- 声学场景分类：使用场景标签（“办公室”、“海滩”、“体育场”）对音频进行标记。\n",
    "- 声学事件检测：使用声音事件标签（“汽车喇叭声”、“鲸鱼叫声”、“玻璃破碎声”）对音频进行标记。\n",
    "- 标记：对包含多种声音的音频进行标记（鸟鸣、会议中的说话人识别）。\n",
    "- 音乐分类：使用流派标签（“金属”、“嘻哈”、“乡村”）对音乐进行标记。\n",
    "\n",
    "\n",
    "```\n",
    "情感识别（ER）为每个话语预测一个情感类别。我们采用了最广泛使用的ER数据集IEMOCAP，并遵循传统的评估协议：我们删除不平衡的情感类别，只保留最后四个具有相似数量数据点的类别，并在标准分割的五折交叉验证上进行评估。评估指标是准确率（ACC）。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c435f1-5af7-46a1-97d8-bf874f288ec9",
   "metadata": {},
   "source": [
    "### 前置依赖包安装\n",
    "\n",
    "建议在命令行安装必要的音频数据处理包: ffmpeg\n",
    "\n",
    "```shell\n",
    "$apt update & apt upgrade\n",
    "$apt install -y ffmpeg\n",
    "$pip install ffmpeg ffmpeg-python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f2d6f28-53bf-4e60-8c34-311d5195099c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.4532, 'label': 'hap'},\n",
       " {'score': 0.3622, 'label': 'sad'},\n",
       " {'score': 0.0943, 'label': 'neu'},\n",
       " {'score': 0.0903, 'label': 'ang'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/superb/hubert-base-superb-er\n",
    "audio_classifier = pipeline(task=\"audio-classification\", model=\"superb/hubert-base-superb-er\")\n",
    "preds = audio_classifier(\"data/audio/mlk.flac\")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5f6a8a25-f38f-4ff2-bb4f-eb66689ed380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/peft/lib/python3.10/site-packages/transformers/configuration_utils.py:365: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at superb/wav2vec2-base-superb-er were not used when initializing Wav2Vec2ForSequenceClassification: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at superb/wav2vec2-base-superb-er and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.8077, 'label': 'sad'},\n",
       " {'score': 0.1082, 'label': 'neu'},\n",
       " {'score': 0.08, 'label': 'hap'},\n",
       " {'score': 0.004, 'label': 'ang'}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/superb/wav2vec2-base-superb-er\n",
    "audio_classifier = pipeline(task=\"audio-classification\", model=\"superb/wav2vec2-base-superb-er\")\n",
    "preds = audio_classifier(\"data/audio/mlk.flac\")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632957dc-5db3-404e-8c24-693ea78b769d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "239b2c71-bc80-4162-8f3c-737c66fc2404",
   "metadata": {},
   "source": [
    "### Automatic speech recognition（ASR）\n",
    "\n",
    "**Automatic speech recognition**（自动语音识别）将语音转录为文本。这是最常见的音频任务之一，部分原因是因为语音是人类交流的自然形式。如今，ASR系统嵌入在智能技术产品中，如扬声器、电话和汽车。我们可以要求虚拟助手播放音乐、设置提醒和告诉我们天气。\n",
    "\n",
    "但是，Transformer架构帮助解决的一个关键挑战是低资源语言。通过在大量语音数据上进行预训练，仅在一个低资源语言的一小时标记语音数据上进行微调，仍然可以产生与以前在100倍更多标记数据上训练的ASR系统相比高质量的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95e7a1b1-c990-4e5f-a155-a6979c945380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/openai/whisper-small\n",
    "transcriber = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n",
    "text = transcriber(\"data/audio/mlk.flac\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce1bd756-342a-464a-829a-bfce466d506a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "transcriber = pipeline(task=\"automatic-speech-recognition\")\n",
    "text = transcriber(\"data/audio/mlk.flac\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d307efab-87b2-433e-93b6-af4c87339036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "201b0f3a-f493-475c-b0cb-b20e16cf1797",
   "metadata": {},
   "source": [
    "## Computer Vision 计算机视觉\n",
    "\n",
    "**Computer Vision**（计算机视觉）任务中最早成功之一是使用卷积神经网络（CNN）识别邮政编码数字图像。图像由像素组成，每个像素都有一个数值。这使得将图像表示为像素值矩阵变得容易。每个像素值组合描述了图像的颜色。\n",
    "\n",
    "计算机视觉任务可以通过以下两种通用方式解决：\n",
    "\n",
    "- 使用卷积来学习图像的层次特征，从低级特征到高级抽象特征。\n",
    "- 将图像分成块，并使用Transformer逐步学习每个图像块如何相互关联以形成图像。与CNN偏好的自底向上方法不同，这种方法有点像从一个模糊的图像开始，然后逐渐将其聚焦清晰。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126afa74-777c-4793-b265-1c05ab321eea",
   "metadata": {},
   "source": [
    "### Image Classificaiton\n",
    "\n",
    "**Image Classificaiton**(图像分类)将整个图像从预定义的类别集合中进行标记。像大多数分类任务一样，图像分类有许多实际用例，其中一些包括：\n",
    "\n",
    "- 医疗保健：标记医学图像以检测疾病或监测患者健康状况\n",
    "- 环境：标记卫星图像以监测森林砍伐、提供野外管理信息或检测野火\n",
    "- 农业：标记农作物图像以监测植物健康或用于土地使用监测的卫星图像\n",
    "- 生态学：标记动物或植物物种的图像以监测野生动物种群或跟踪濒危物种"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbb3af2-ddd7-412f-acff-9d856804073c",
   "metadata": {},
   "source": [
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\n",
    "![](data/image/cat-chonk.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e35cf1b-dc22-4d56-a1b1-dc372a18b324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.4335, 'label': 'lynx, catamount'}\n",
      "{'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}\n",
      "{'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}\n",
      "{'score': 0.0239, 'label': 'Egyptian cat'}\n",
      "{'score': 0.0229, 'label': 'tiger cat'}\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/google/vit-base-patch16-224\n",
    "image_classifier = pipeline(task=\"image-classification\")\n",
    "preds = image_classifier(\n",
    "    \"data/image/cat-chonk.jpeg\"\n",
    ")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "print(*preds, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "58de9bcf-45b8-410e-8fb9-f16b7f3910f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf98278128749829834be2d53302443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8568acae6747b5974f42ab4b95a5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f0913210174e54a3d21662a1d15811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/266 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.6538, 'label': 'lynx, catamount'}\n",
      "{'score': 0.0661, 'label': 'tabby, tabby cat'}\n",
      "{'score': 0.034, 'label': 'tiger cat'}\n",
      "{'score': 0.0279, 'label': 'wombat'}\n",
      "{'score': 0.0251, 'label': 'wood rabbit, cottontail, cottontail rabbit'}\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/microsoft/resnet-18\n",
    "image_classifier = pipeline(task=\"image-classification\", model=\"microsoft/resnet-18\")\n",
    "preds = image_classifier(\n",
    "    \"data/image/cat-chonk.jpeg\"\n",
    ")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "print(*preds, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eea201-8c19-4ac6-98cc-63d3c63c0125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4839c6b-8492-452f-bd87-6a95144d7d4f",
   "metadata": {},
   "source": [
    "### Object Detection\n",
    "\n",
    "与图像分类不同，目标检测在图像中识别多个对象以及这些对象在图像中的位置（由边界框定义）。目标检测的一些示例应用包括：\n",
    "\n",
    "- 自动驾驶车辆：检测日常交通对象，如其他车辆、行人和红绿灯\n",
    "- 遥感：灾害监测、城市规划和天气预报\n",
    "- 缺陷检测：检测建筑物中的裂缝或结构损坏，以及制造业产品缺陷\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e3903f-ed3c-4ee1-8233-a5e26db58cd3",
   "metadata": {},
   "source": [
    "#### 前置依赖包安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec283503-2424-40d6-a660-e575c11c23ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddb1a10-b12a-4032-8bd0-d492d3790d60",
   "metadata": {},
   "source": [
    "![](data/image/cat_dog.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "856a7513-c638-445d-8555-f5c1227b97b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/detr-resnet-50 and revision 2729413 (https://huggingface.co/facebook/detr-resnet-50).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9985,\n",
       "  'label': 'cat',\n",
       "  'box': {'xmin': 78, 'ymin': 57, 'xmax': 309, 'ymax': 371}},\n",
       " {'score': 0.989,\n",
       "  'label': 'dog',\n",
       "  'box': {'xmin': 279, 'ymin': 20, 'xmax': 482, 'ymax': 416}}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/facebook/detr-resnet-50\n",
    "detector = pipeline(task=\"object-detection\")\n",
    "preds = detector(\n",
    "    \"data/image/cat_dog.jpg\"\n",
    ")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"], \"box\": pred[\"box\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "536bdf60-6eba-4a65-8285-4d90ea959f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.996,\n",
       "  'label': 'dog',\n",
       "  'box': {'xmin': 284, 'ymin': 22, 'xmax': 486, 'ymax': 415}},\n",
       " {'score': 0.9896,\n",
       "  'label': 'cat',\n",
       "  'box': {'xmin': 73, 'ymin': 66, 'xmax': 295, 'ymax': 372}}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/hustvl/yolos-small\n",
    "detector = pipeline(task=\"object-detection\", model=\"hustvl/yolos-small\")\n",
    "preds = detector(\n",
    "    \"data/image/cat_dog.jpg\"\n",
    ")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"], \"box\": pred[\"box\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7c89e9-ec8b-47e8-b7ee-22ecbba120bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
